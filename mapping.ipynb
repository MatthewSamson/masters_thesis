{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package section..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from math import sqrt\n",
    "from collections import defaultdict, Counter\n",
    "from sys import stdout\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "print(\"Packages successfully imported!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. File loaders..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = \"pre_files\"\n",
    "norm = \"norm_files\"\n",
    "init_wt = \"init\" # not needed\n",
    "trained = \"weights\"\n",
    "\n",
    "# load a file as a dataframe\n",
    "def csv_files(loc, name):\n",
    "    \n",
    "    os.chdir(loc)\n",
    "    pdx = pd.read_csv(name)\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "    # remove extra enumerated column\n",
    "    return pdx.iloc[:, 1:]\n",
    "\n",
    "\n",
    "# load a file as an numpy file\n",
    "def npy_files(loc, name):\n",
    "    \n",
    "    os.chdir(loc)\n",
    "    npx = np.load(name)\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "    return npx\n",
    "\n",
    "\n",
    "# create a dictionary of required load functions\n",
    "fload_dict = dict()\n",
    "fload_dict['csv_files'] = csv_files\n",
    "fload_dict['npy_files'] = npy_files\n",
    "\n",
    "\n",
    "# dictionary creation and lookup functions\n",
    "def lookup(cur_dict):\n",
    "    \n",
    "    keyset = list(cur_dict.keys())\n",
    "    valset = list(cur_dict.values())\n",
    "    \n",
    "    # look into the dictionary\n",
    "    '''for i in range(len(keyset)):\n",
    "        print(keyset[i], ' ---> ', valset[i])'''\n",
    "        \n",
    "    return keyset, valset\n",
    "\n",
    "# join up a dictionary\n",
    "def join_dict(keyset, valset):\n",
    "    \n",
    "    new_dict = dict()\n",
    "    for i in range(len(keyset)):\n",
    "        new_dict[keyset[i]] = valset[i]\n",
    "        \n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# load the list of file names\n",
    "def file_list(direc, ext):\n",
    "    \n",
    "    # gathering the files just in case\n",
    "    list_files = []\n",
    "        \n",
    "    os.chdir(direc)\n",
    "    print(os.getcwd())\n",
    "    \n",
    "    f_ext = \"*.\" + ext\n",
    "    for files in glob.glob(f_ext):\n",
    "        list_files.append(files)\n",
    "        \n",
    "    os.chdir('..')\n",
    "    print(list_files)\n",
    "    print(os.getcwd())\n",
    "    print(\"*****************\\n\")\n",
    "    return list_files\n",
    "\n",
    "\n",
    "# return dictionaries of file list\n",
    "def file_dict(f_list, loc, f_type):\n",
    "    \n",
    "    # empty list to store each file_data\n",
    "    f_store = []\n",
    "    \n",
    "    # deal with removing e\n",
    "    # load each file_data\n",
    "    for fn in f_list:\n",
    "        # check the file extension\n",
    "        loader = fload_dict[f_type]\n",
    "        fi = loader(loc, fn)    \n",
    "        f_store.append(fi)\n",
    "        \n",
    "    print(\"Num of files: \", len(f_list), \"\\nNum of files loaded: \", len(f_store))\n",
    "    \n",
    "    # dictionary to store file_names : file_data\n",
    "    f_dict = join_dict(f_list, f_store)\n",
    "        \n",
    "    print(\"Dictionary compiled!!\", len(f_dict.keys()))\n",
    "    return f_dict\n",
    "\n",
    "\n",
    "# load up the index from a csv file\n",
    "def load_idx(fnames):\n",
    "    \n",
    "    import csv\n",
    "    fvals = []\n",
    "    os.chdir('mapping')\n",
    "    \n",
    "    for fname in fnames:\n",
    "        with open(fname, 'rt') as f:\n",
    "            cur_csv = csv.reader(f)\n",
    "            idx = defaultdict(list)\n",
    "            \n",
    "            for line in cur_csv:\n",
    "                for val in range(3, int(line[2])+3):\n",
    "\n",
    "                    k1 = int(line[0])\n",
    "                    k2 = int(line[1])\n",
    "                    vv = int(line[val])\n",
    "\n",
    "                    idx[k1, k2].append(vv)\n",
    "                    \n",
    "        print(\"File \", fname, \" has been loaded\")\n",
    "        fvals.append(idx)\n",
    "\n",
    "    os.chdir('..')\n",
    "    \n",
    "    fin_dict = join_dict(fnames, fvals)\n",
    "    return fin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply file and dictionary loaders..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of file names\n",
    "\n",
    "# get the list of csv files for the dataframes\n",
    "csv_files = file_list(pre, \"csv\")\n",
    "\n",
    "# get the list of npy files for each normalised array\n",
    "npy_files = file_list(norm, \"npy\")\n",
    "\n",
    "# load up the trained weights data file names\n",
    "wt_files = file_list(trained, \"npy\")\n",
    "\n",
    "# load up the resultant mappings\n",
    "map_files = file_list(\"mapping\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the dictionary of csv dataframe file names to file dataframe data\n",
    "df_csv = file_dict(csv_files, pre, \"csv_files\")\n",
    "\n",
    "# set up the dictionary of npy numpy array file names to file numpy data\n",
    "num_npy = file_dict(npy_files, norm, \"npy_files\")\n",
    "\n",
    "# load the trained weights data files \n",
    "tr_wt = file_dict(wt_files, trained, \"npy_files\")\n",
    "\n",
    "# setting up all the mappings\n",
    "tr_map = load_idx(map_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_wt['ten_re_wt.npy'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_map.keys(), tr_wt.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting attempts for thresholding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sort values and find the thresh point\n",
    "def plot_sort(df_csv, field):\n",
    "    df_k, df_v = lookup(df_csv)\n",
    "    new_k, new_v = df_k, []\n",
    "    \n",
    "    for i in range(len(df_k)):\n",
    "        cur_k = df_k[i]\n",
    "        cur_v = df_v[i]\n",
    "        \n",
    "        print(\"Data of shape: \", cur_v.shape, \" plotting begins...\")\n",
    "        # sorting here\n",
    "        new_df = cur_v.sort_values(field)\n",
    "        cur_fi = new_df[field]\n",
    "        new_v.append(new_df)\n",
    "        \n",
    "        print(\"Sorting \", cur_k, \" by \", field)\n",
    "        for i in range(cur_fi.shape[0]):\n",
    "            if(i%100  == 0):\n",
    "                plt.plot(i, cur_fi.iloc[i], 'r.')\n",
    "        plt.show()\n",
    "        \n",
    "    sort_di = join_dict(new_k, new_v)\n",
    "    return sort_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "field_li = list(df_csv['al.csv'].columns.values)\n",
    "sort_k, sort_v = [], []\n",
    "\n",
    "for i in range(1, len(field_li)):\n",
    "    sort_k.append(field_li[i])\n",
    "    vv = plot_sort(df_csv, field_li[i])\n",
    "    sort_v.append(vv)\n",
    "\n",
    "sorted_di = join_dict(sort_k, sort_v)\n",
    "sorted_di.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### helper functions\n",
    "\n",
    "# function to oberve and print shapes within each dictionary\n",
    "def dict_shape(cur_dict):\n",
    "    \n",
    "    key = list(cur_dict.keys())\n",
    "    for k in key:\n",
    "        s = cur_dict[k].shape\n",
    "        print(\"Shape of \", k, \": \", s)\n",
    "    return\n",
    "\n",
    "# function to find the number of junctions\n",
    "def junc(pdx):\n",
    "    # pdx: dataframe used to find out what label to apply\n",
    "    \n",
    "    junc_pts = []\n",
    "    \n",
    "    for i in range(pdx.shape[0]):\n",
    "        # compare using the starting time\n",
    "        start = pdx.iloc[0, 0]\n",
    "        \n",
    "        if pdx.iloc[i, 0] == start:\n",
    "            junc_pts.append(i)\n",
    "            \n",
    "    junc_pts.append(pdx.shape[0])\n",
    "    return junc_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_lab(df_dict, field, thr1, thr2):\n",
    "    df_k, df_v = lookup(df_dict)\n",
    "    la_k, la_v = [], []\n",
    "    \n",
    "    for i in range(len(df_k)):\n",
    "        cur_k, cur_v = df_k[i], df_v[i]\n",
    "        lab = np.zeros(pdx.shape[0], dtype='int').reshape(-1, 1)\n",
    "\n",
    "        for i in range(pdx.shape[0]):\n",
    "            if pdx[field][i] > thr:\n",
    "                lab[i] = 1\n",
    "            else:\n",
    "                lab[i] = 0\n",
    "\n",
    "        print(\"Labels of \", field, \" of \", cur_k, \" has been created: \", lab.shape)\n",
    "        lab_n = cur_k.rstrip(\".csv\") + \"_\" + field\n",
    "        la_k.append(lab_n)\n",
    "        la_v.append(lab)\n",
    "        \n",
    "    fin_dict = join_dict(la_k, la_v)\n",
    "    return fin_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brake label! --> Brake Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brake_lab(pdx, df_dict):\n",
    "    # pdx: only uses dataframes type of data\n",
    "\n",
    "    lab = np.zeros(pdx.shape[0], dtype='int').reshape(-1, 1)\n",
    "    \n",
    "    for i in range(pdx.shape[0]):\n",
    "        if pdx['Brake pressure/Axle 1/Left'][i] > 0.01:\n",
    "            lab[i] = 1\n",
    "            \n",
    "        else:\n",
    "            lab[i] = 0\n",
    "            \n",
    "    print(\"Labels of 1D created based on left right acceleration...\", lab.shape)\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed label! --> Tangential Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['al.csv']['Tangential speed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## threshold used, not for detection ##\n",
    "def speed_lab(pdx, df_dict):\n",
    "    # pdx: only uses dataframes type of data\n",
    "\n",
    "    lab = np.zeros(pdx.shape[0], dtype='int').reshape(-1, 1)\n",
    "    \n",
    "    for i in range(pdx.shape[0]):\n",
    "        # above thresh - high speed\n",
    "        if pdx['Tangential speed'][i] > 76:\n",
    "            lab[i] = 1\n",
    "        # below thresh - lesser than high\n",
    "        else:\n",
    "            lab[i] = 0\n",
    "            \n",
    "    print(\"Labels of 1D created based on left right acceleration...\", lab.shape)\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Front/Back label! --> Tangent Acceleration X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['al.csv']['Tangent Acceleration/X'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## threshold used, not for detection ##\n",
    "def froba_lab(pdx, df_dict):\n",
    "    # pdx: only uses dataframes type of data\n",
    "\n",
    "    lab = np.zeros(pdx.shape[0], dtype='int').reshape(-1, 1)\n",
    "    \n",
    "    for i in range(pdx.shape[0]):\n",
    "        # forward motion\n",
    "        if pdx['Tangent Acceleration/X'][i] > 0:\n",
    "            lab[i] = 1\n",
    "        # backward motion\n",
    "        else:\n",
    "            lab[i] = 0\n",
    "            \n",
    "    print(\"Labels of 1D created based on left right acceleration...\", lab.shape)\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Left/Right label! --> Tangent Acceleration Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['al.csv']['Tangent Acceleration/Y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return 0 as -ve and 1 as +ve for acceleration along y\n",
    "def lefri_lab(pdx, df_dict):\n",
    "    # pdx: only uses dataframes type of data\n",
    "    \n",
    "    lab = np.zeros(pdx.shape[0], dtype='int').reshape(-1, 1)\n",
    "    \n",
    "    for i in range(pdx.shape[0]):\n",
    "        if pdx['Tangent Acceleration/Y'][i] > 0:\n",
    "            # +ve direction\n",
    "            lab[i] = 1\n",
    "        else:\n",
    "            # -ve direction\n",
    "            lab[i] = 0\n",
    "            \n",
    "    print(\"Labels of 1D created based on braking...\", lab.shape)\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top/Bottom label! --> Tangent Acceleration Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['al.csv']['Tangent Acceleration/Z'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return 0 as -ve and 1 as +ve for acceleration along y\n",
    "def topbot_lab(pdx, df_dict):\n",
    "    # pdx: only uses dataframes type of data\n",
    "    \n",
    "    lab = np.zeros(pdx.shape[0], dtype='int').reshape(-1, 1)\n",
    "    \n",
    "    for i in range(pdx.shape[0]):\n",
    "        if pdx['Tangent Acceleration/Z'][i] > 0:\n",
    "            # top direction?\n",
    "            lab[i] = 1\n",
    "        else:\n",
    "            # bottom direction?\n",
    "            lab[i] = 0\n",
    "            \n",
    "    print(\"Labels of 1D created based on braking...\", lab.shape)\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Gap label?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['al.csv']['Lane gap'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return 0 as -ve and 1 as +ve for acceleration along y\n",
    "def langap_lab(pdx, df_dict):\n",
    "    # pdx: only uses dataframes type of data\n",
    "    \n",
    "    lab = np.zeros(pdx.shape[0], dtype='int').reshape(-1, 1)\n",
    "    \n",
    "    for i in range(pdx.shape[0]):\n",
    "        if pdx['Lane gap'][i] > 0.077:\n",
    "            # +ve lane gap?\n",
    "            lab[i] = 1\n",
    "        else:\n",
    "            # -ve lane gap?\n",
    "            lab[i] = 0\n",
    "            \n",
    "    print(\"Labels of 1D created based on braking...\", lab.shape)\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split label - divide into distract & driver!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to place driver label on data\n",
    "def split_lab(pdx, df_dict):\n",
    "    # pdx: dataframe used to find out what label to apply\n",
    "    \n",
    "    # initialising the final label array\n",
    "    label = np.zeros(pdx.shape[0], dtype='int').reshape(-1, 1)\n",
    "    \n",
    "    # get the junction points and create the label\n",
    "    junctions = junc(pdx)\n",
    "    tot = len(junctions)\n",
    "    lab = 0 # the first label\n",
    "    \n",
    "    for i in range(tot-1):\n",
    "        start = junctions[i]\n",
    "        end = junctions[i + 1]\n",
    "        \n",
    "        # label the split set\n",
    "        for i in range(start, end):\n",
    "            label[i] = lab\n",
    "        \n",
    "        # go to next label\n",
    "        lab = lab + 1\n",
    "    \n",
    "    print(junctions)\n",
    "    print(\"Labels of 1D created based on split data...\", label.shape)\n",
    "    print(\"The labels are: \", set(label[:, 0]))\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distraction label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to place driver label on data\n",
    "def distract_lab(pdx):\n",
    "    # pdx: dataframe used to find out what label to apply\n",
    "    \n",
    "    # initialising the final label array\n",
    "    label = np.zeros(pdx.shape[0], dtype='int').reshape(-1, 1)\n",
    "    \n",
    "    # get the junction points and create the label\n",
    "    junctions = junc(pdx)\n",
    "    tot = len(junctions)\n",
    "    lab = -1 # the first label\n",
    "    \n",
    "    for i in range(tot-1):\n",
    "        start = junctions[i]\n",
    "        end = junctions[i + 1]\n",
    "        \n",
    "        if i%40 == 0:\n",
    "            # go to next label\n",
    "            lab = lab + 1\n",
    "        \n",
    "        # label the split set\n",
    "        for j in range(start, end):\n",
    "            label[j] = lab\n",
    "           \n",
    "    print(junctions)\n",
    "    print(\"Labels of 1D created based on split data...\", label.shape)\n",
    "    print(\"Testing labels:\")\n",
    "    for jj in junctions:\n",
    "        print(label[jj])\n",
    "    print(\"The labels are: \", set(label[:, 0]))\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment label !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to segment data into parts\n",
    "def seg_lab(pdx, df_dict):\n",
    "    # data: any whole set of data\n",
    "    # label: only !!driver!! label\n",
    "    \n",
    "    \n",
    "    # Step 1: initialise all required variables\n",
    "    low = 0\n",
    "    # initialise a label array to return\n",
    "    new_lab = np.zeros(pdx.shape[0], dtype='int').reshape(-1, 1)\n",
    "    # initialise number of segments (including 0, so 0 to 4)\n",
    "    segments = 5\n",
    "    # initialise segment array\n",
    "    seg_arr = np.arange(segments, dtype='int')\n",
    "    print(type(seg_arr[0]))\n",
    "    \n",
    "    # initialise points of driver split\n",
    "    splits = junc(pdx)\n",
    "    print(\"The splits are at: \", splits)\n",
    "    \n",
    "    \n",
    "    # Step 3: segment data into partitions by individual driver\n",
    "    # pick out each single driver\n",
    "    for i in range(len(splits) - 1):\n",
    "        start = splits[i]\n",
    "        end = splits[i + 1]\n",
    "        tot_pts = end - start\n",
    "        limit = int(tot_pts / (segments)) # number of segments\n",
    "        up = low\n",
    "        no = 0\n",
    "        print(\"Start: \", start, \" End: \", end, \" Pts in b/w: \", tot_pts, \"Pts per seg: \", limit)\n",
    "        \n",
    "        # segment signal driver\n",
    "        for pt in range(start, end):\n",
    "            \n",
    "            # deal with ending stuff\n",
    "            if no == segments: # set out of limit label\n",
    "                new_lab[pt] = seg_arr[seg_arr.shape[0]-1]\n",
    "                \n",
    "            else:\n",
    "                # regular 0 to 4 segments label\n",
    "                new_lab[pt] = seg_arr[no]\n",
    "                up = up + 1 # note the number in segement\n",
    "\n",
    "                if up == limit:\n",
    "                    up = low # reset up\n",
    "                    no = no + 1 # new label\n",
    "                \n",
    "    return new_lab                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create label map dictionary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary for what label creating function to be used\n",
    "\n",
    "flab_dict = dict()\n",
    "flab_dict['br_lab'] = brake_lab\n",
    "flab_dict['sd_lab'] = speed_lab\n",
    "flab_dict['fb_lab'] = froba_lab\n",
    "flab_dict['lr_lab'] = lefri_lab\n",
    "flab_dict['tb_lab'] = topbot_lab\n",
    "flab_dict['lg_lab'] = langap_lab\n",
    "flab_dict['sp_lab'] = split_lab\n",
    "flab_dict['se_lab'] = seg_lab\n",
    "\n",
    "# insert other functions here\n",
    "\n",
    "\n",
    "\n",
    "# reset names between different types of files and what they are about\n",
    "def reset_name(flist, rear):\n",
    "    # flist: list of current file names (usually csv files)\n",
    "    # lab_name: name of the label function\n",
    "    # front: new appending character to denote change in file names (at start part)\n",
    "    # rear: new file extension required (at end part)\n",
    "    \n",
    "    new_f = []\n",
    "    \n",
    "    for fi in flist:\n",
    "        # change the first letter (new file type) (first letter)\n",
    "        name = ''\n",
    "        \n",
    "        # keep adding letters upto '.'\n",
    "        for idx in range(len(fi)):\n",
    "            if fi[idx] == '.':\n",
    "                break\n",
    "            name = name + fi[idx]\n",
    "        \n",
    "        # add the new extension\n",
    "        name = name + rear\n",
    "        \n",
    "        # complete by adding the new name to the list\n",
    "        new_f.append(name)\n",
    "        \n",
    "    print(new_f)\n",
    "    return new_f\n",
    "\n",
    "\n",
    "# creating functions that return a dictionary of a particular label\n",
    "def lab_dict(cur_dict, rear, lab_name):\n",
    "    # cur_dict: dataframe dictionary\n",
    "    # front: new appending character to denote change in file names\n",
    "    # rear: new file extension required (at end part)\n",
    "    # lab_name: the type of label being used\n",
    "    \n",
    "    # current keys\n",
    "    cur_k = list(cur_dict.keys())\n",
    "    \n",
    "    # current values\n",
    "    cur_val = list(cur_dict.values())\n",
    "    \n",
    "    # next key set\n",
    "    nex_k = reset_name(cur_k, rear)\n",
    "    \n",
    "    # next value set\n",
    "    nex_val = []\n",
    "    \n",
    "    # set label function\n",
    "    labeler = flab_dict[lab_name]\n",
    "    \n",
    "    # creating the next dictionary\n",
    "    for val in cur_val:\n",
    "        nex = labeler(val, cur_dict)\n",
    "        nex_val.append(nex)\n",
    "        \n",
    "    print(\"Len of new keys created..\", len(nex_k))\n",
    "    print(\"Len of new values created..\", len(nex_val))\n",
    "    \n",
    "    # create the dictionary of new keys and values\n",
    "    nex_dict = join_dict(nex_k, nex_val)\n",
    "        \n",
    "    return nex_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the label functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create brake label using only the dataframe data\n",
    "br_lab = lab_dict(df_csv, \"_brl\", \"br_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create speed label using only the dataframe data\n",
    "sd_lab = lab_dict(df_csv, \"_sdl\", \"sd_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create axis x label using only the dataframe data\n",
    "fb_lab = lab_dict(df_csv, \"_fbl\", \"fb_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create axis y label using only the dataframe data\n",
    "lr_lab = lab_dict(df_csv, \"_lrl\", \"lr_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create axis z label using only the dataframe data\n",
    "tb_lab = lab_dict(df_csv, \"_tbl\", \"tb_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lane gap label using only the dataframe data\n",
    "lg_lab = lab_dict(df_csv, \"_lgl\", \"lg_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create split driver label using only the dataframe data\n",
    "sp_lab = lab_dict(df_csv, \"_spl\", \"sp_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di_lab = distract_lab(df_csv['al.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment label x2 using only the dataframe data\n",
    "s2_lab = lab_dict(df_csv, \"_s2l\", \"se_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment label x3 using only the dataframe data\n",
    "s3_lab = lab_dict(df_csv, \"_s3l\", \"se_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment label x4 using only the dataframe data\n",
    "s4_lab = lab_dict(df_csv, \"_s4l\", \"se_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment label x5 using only the dataframe data\n",
    "s5_lab = lab_dict(df_csv, \"_s5l\", \"se_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment label x6 using only the dataframe data\n",
    "s6_lab = lab_dict(df_csv, \"_s6l\", \"se_lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment label x7 using only the dataframe data\n",
    "s7_lab = lab_dict(df_csv, \"_s7l\", \"se_lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further lab mod (unexpected)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the modification here\n",
    "def reset_1lab(val0, br):\n",
    "    # lab sent is a 1d vector\n",
    "    new_lab = np.zeros(val0.shape[0], dtype='int').reshape(val0.shape[0], -1)\n",
    "    cur_lab = 0\n",
    "    new_pt = 1\n",
    "    nex = 0\n",
    "    \n",
    "    for l in range(val0.shape[0]):\n",
    "        vv = val0[l, 0]\n",
    "        if vv != cur_lab:\n",
    "            cur_lab = vv\n",
    "            new_pt = new_pt + 1\n",
    "            \n",
    "        if new_pt % br == 0:\n",
    "            nex = nex + 1\n",
    "            new_pt = 1\n",
    "            \n",
    "        new_lab[l, 0] = nex\n",
    "    return new_lab\n",
    "\n",
    "\n",
    "# call the mod on any label\n",
    "def mod_1lab(lab_dict, mod_key):\n",
    "    \n",
    "    keys, values = lookup(lab_dict)\n",
    "    mod_dict = lab_dict.copy()\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        for j in range(len(mod_key)):\n",
    "            if keys[i] == mod_key[j]:\n",
    "                mod_dict[keys[i]] = reset_1lab(values[i], 4)\n",
    "                break\n",
    "    \n",
    "    return mod_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Mapping window labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the mid threshold\n",
    "def get_mid(midded):\n",
    "    mid = int(midded.shape[1]/2)\n",
    "    midthr = midded[:, mid]\n",
    "    return midthr\n",
    "\n",
    "# function to remap data in the form of a window\n",
    "# !! works for both data and labels!!\n",
    "def remap(data, window):\n",
    "    \n",
    "    x = data.shape[0] - window\n",
    "    y = data.shape[1] * window\n",
    "    \n",
    "    if data.shape[1] == 1:\n",
    "        remapped = np.zeros(x * y, dtype='int').reshape(x, y)\n",
    "        \n",
    "    else:\n",
    "        remapped = np.zeros(x * y).reshape(x, y)\n",
    "        \n",
    "\n",
    "    j = 0 # final map setter\n",
    "    \n",
    "    for idx in range(0, data.shape[0]-window-1):\n",
    "        \n",
    "        vec = np.zeros(y).reshape(window, data.shape[1])\n",
    "        i = 0 # each single vector\n",
    "        \n",
    "        for ar_id in range(idx, idx + window):\n",
    "            vec[i, :] = data[ar_id, :]\n",
    "            i = i + 1\n",
    "            \n",
    "        n_vec = vec.reshape(-1)\n",
    "        \n",
    "        remapped[j, :] = n_vec\n",
    "        j = j + 1\n",
    "        \n",
    "    print(\"Label shape after window: \", remapped.shape)\n",
    "    \n",
    "    remap_mid = get_mid(remapped)\n",
    "    return remap_mid\n",
    "\n",
    "# remapping is required only for labels as weights are already trained on the windowed input data\n",
    "\n",
    "# function to quickly create a dictionary\n",
    "def remap_dict(cur_dict, window):\n",
    "    # cur_dict: the dictionary of data to be windowed\n",
    "    # window: the size of the window\n",
    "    \n",
    "    win_values = []\n",
    "    \n",
    "    cur_keys = list(cur_dict.keys())\n",
    "    print(cur_keys)\n",
    "    \n",
    "    for val in cur_dict.values():\n",
    "        win_val = remap(val, window)\n",
    "        win_values.append(win_val)\n",
    "        \n",
    "    ret_dict = dict()\n",
    "    for i in range(len(cur_keys)):\n",
    "        ret_dict[cur_keys[i]] = win_values[i]\n",
    "    \n",
    "    print(\"Len of new set of values: \", len(win_values))\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply remapping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating the windowed brake dictionary\n",
    "br_remap = remap_dict(br_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed speed dictionary\n",
    "sd_remap = remap_dict(sd_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed axis x dictionary\n",
    "fb_remap = remap_dict(fb_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed axis y dictionary\n",
    "lr_remap = remap_dict(lr_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed axis z dictionary\n",
    "tb_remap = remap_dict(tb_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed lane gap dictionary\n",
    "lg_remap = remap_dict(lg_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed  dictionary\n",
    "sp_remap = remap_dict(sp_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di_remap = remap(di_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed split driver dictionary\n",
    "s2_remap = remap_dict(s2_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed split driver dictionary\n",
    "s3_remap = remap_dict(s3_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed split driver dictionary\n",
    "s4_remap = remap_dict(s4_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed split driver dictionary\n",
    "s5_remap = remap_dict(s5_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed split driver dictionary\n",
    "s6_remap = remap_dict(s6_lab, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the windowed split driver dictionary\n",
    "s7_remap = remap_dict(s7_lab, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load up the weights..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-set file names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all required file locations\n",
    "hf = \"hf_res\"\n",
    "mus = \"mus_res\"\n",
    "tex = \"tex_res\"\n",
    "com = \"com_res\"\n",
    "first = \"1st_res\"\n",
    "second = \"2nd_res\"\n",
    "brake = \"br_res\"\n",
    "\n",
    "# sub file locations\n",
    "ntime = 'no_time'\n",
    "wtime = 'with_time'\n",
    "mth = 'max_thresh'\n",
    "nth = 'no_thresh'\n",
    "\n",
    "# file types\n",
    "csv = \"csv\"\n",
    "wt = \"npy\"\n",
    "img = \"png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Thresholding the labels and other helpers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thresholding Functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the thresholded value based on the maximum count in each cluster\n",
    "def plot_thresh(lab, ind, max_size, thresh_name, loc, save):\n",
    "    # lab: 1d labels\n",
    "    # ind: map to list index\n",
    "    # thresh_name: add an extra name\n",
    "    # loc: where to store the results\n",
    "    # save: do you want to save the image (True or False)\n",
    "    \n",
    "    \n",
    "    # set the markers and colors\n",
    "    marker = ['o', '*', 's', 'D', '^', '+', 'X', '1', 'p', '>', 'x', 'p']\n",
    "    color = ['r', 'g', 'b', 'y', 'k', 'm', 'c', 'y', 'r', 'g', 'b', 'y']\n",
    "    \n",
    "    plt.axis([0, max_size, 0, max_size])\n",
    "    \n",
    "    print(\"Markers: \", marker)\n",
    "    print(\"Colours: \", color)\n",
    "    print()\n",
    "    \n",
    "    cn = 0\n",
    "    \n",
    "    # open the map dictionary\n",
    "    for key in ind.keys():\n",
    "        \n",
    "        # deal only with key having something in the list\n",
    "        if(len(ind[key]) != 0):\n",
    "            #print(key, \" -- \", ind[key])\n",
    "            #print(\"===============\")\n",
    "            lis = [0] * (len(set(lab)))\n",
    "            \n",
    "            # open a point in the list\n",
    "            for pt in ind[key]:\n",
    "                \n",
    "                # deal only with points in the required label limit\n",
    "                if pt < lab.shape[0]: \n",
    "                    res = lab[pt]\n",
    "                    lis[res] = lis[res] + 1\n",
    "                    maxi = max(lis)\n",
    "            #print(lis)\n",
    "            \n",
    "            # locate the index of the maximum value and plot only that\n",
    "            for i in range(len(lis)):\n",
    "                if lis[i] == maxi:\n",
    "                    #print(\"MAX: \", maxi)\n",
    "                    pts = i\n",
    "                    #print(\"MAX lab: \", pt)\n",
    "\n",
    "            plt.plot(key[0]+0.5, key[1]+0.5, marker[pts], markerfacecolor='None', \n",
    "                            markeredgecolor=color[pts], markersize=4, markeredgewidth=1)\n",
    "            \n",
    "            '''if cn % 10000 == 0:\n",
    "                print(\"Done mapping \", cn, \" no. of points\")\n",
    "            cn = cn + 1'''\n",
    "    \n",
    "    plt.title(thresh_name)\n",
    "    \n",
    "    # save the labeled map\n",
    "    if save == True:\n",
    "        os.chdir(loc)\n",
    "        plt.savefig(thresh_name)\n",
    "        os.chdir('..')\n",
    "        print(\"map saved!!\")\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# plot the thresholded value based on the maximum count in each cluster\n",
    "def plot_thrline(lab, ind, max_size, thresh_name, loc, save):\n",
    "    # lab: 1d labels\n",
    "    # ind: map to list index\n",
    "    # thresh_name: add an extra name\n",
    "    # loc: where to store the results\n",
    "    # save: do you want to save the image (True or False)\n",
    "    \n",
    "    \n",
    "    tot_lab = len(set(lab))\n",
    "    colorlis = np.linspace(0, 0.85, tot_lab)\n",
    "    \n",
    "    plt.axis([0, max_size, 0, max_size])\n",
    "    \n",
    "    cn = 0\n",
    "    \n",
    "    # open the map dictionary\n",
    "    for key in ind.keys():\n",
    "        \n",
    "        # deal only with key having something in the list\n",
    "        if(len(ind[key]) != 0):\n",
    "            #print(key, \" -- \", ind[key])\n",
    "            #print(\"===============\")\n",
    "            lis = [0] * (len(set(lab)))\n",
    "            \n",
    "            # open a point in the list\n",
    "            for pt in ind[key]:\n",
    "                \n",
    "                # deal only with points in the required label limit\n",
    "                if pt < lab.shape[0]: \n",
    "                    res = lab[pt]\n",
    "                    lis[res] = lis[res] + 1\n",
    "                    maxi = max(lis)\n",
    "            #print(lis)\n",
    "            \n",
    "            # locate the index of the maximum value and plot only that\n",
    "            for i in range(len(lis)):\n",
    "                if lis[i] == maxi:\n",
    "                    #print(\"MAX: \", maxi)\n",
    "                    pts = i\n",
    "                    #print(\"MAX lab: \", pt)\n",
    "\n",
    "            plt.plot(key[0]+0.5, key[1]+0.5, '.', markerfacecolor='None', \n",
    "                            markeredgecolor=str(colorlis[pts]), markersize=4, markeredgewidth=1)\n",
    "            \n",
    "            '''if cn % 10000 == 0:\n",
    "                print(\"Done mapping \", cn, \" no. of points\")\n",
    "            cn = cn + 1'''\n",
    "    \n",
    "    plt.title(thresh_name)\n",
    "    \n",
    "    # save the labeled map\n",
    "    if save == True:\n",
    "        os.chdir(loc)\n",
    "        plt.savefig(thresh_name)\n",
    "        os.chdir('..')\n",
    "        print(\"map saved!!\")\n",
    "        \n",
    "    plt.show()\n",
    "     \n",
    "        \n",
    "# plot the thresholded value based on the maximum count in each cluster\n",
    "def plot_thrcolor(lab, ind, max_size, thresh_name, loc, save):\n",
    "    # lab: 1d labels\n",
    "    # ind: map to list index\n",
    "    # thresh_name: add an extra name\n",
    "    # loc: where to store the results\n",
    "    # save: do you want to save the image (True or False)\n",
    "    \n",
    "    split = 10\n",
    "    print(split)\n",
    "    \n",
    "    #color list for 40 labels\n",
    "    '''\n",
    "    colorlis = ['#02b058', '#042333', '#0841fa', '#11fd99', '#28f934', '#3636ee', '#3c91a7', '#3e3986', '#3e8ae0', '#419b63', \n",
    "                '#427b34', '#5c1c3a', '#649b7c', '#65eb69', '#74470b', '#77793c', '#7fbfd4', '#806be1', '#820022', '#a07475',\n",
    "                '#a0f8fb', '#a3f96c', '#a4c00f', '#acf837', '#bd25f4', '#c27495', '#c55a2d', '#c5c8fc', '#c9dc9b', '#d18b2c',\n",
    "                '#dcd6e7', '#e2fa2b', '#e8390a', '#edda36', '#ee5b21', '#f06752', '#f28cf0', '#f68e2f', '#fc1cc0', '#fed3ca']\n",
    "    '''\n",
    "    \n",
    "    #color list for 20 labels\n",
    "    \n",
    "    colorlis = ['#0841fa', '#11fd99', '#28f934', '#042333', '#74470b', '#a07475', '#a4c00f', '#bd25f4', '#d18b2c', '#e8390a']\n",
    "    \n",
    "    print(\"legend: \")\n",
    "    for i in range(len(colorlis)):\n",
    "        print(\"Lab: \", i, \" Color: \", colorlis[i])\n",
    "        \n",
    "    cpt = 0\n",
    "    for y in range(5):\n",
    "        for x in range(2):\n",
    "            plt.plot(x, y, 's', color=colorlis[cpt])\n",
    "            cpt = cpt + 1\n",
    "    plt.show()\n",
    "        \n",
    "    plt.axis([0, max_size, 0, max_size])\n",
    "    \n",
    "    cn = 0\n",
    "    \n",
    "    # open the map dictionary\n",
    "    for key in ind.keys():\n",
    "        \n",
    "        # deal only with key having something in the list\n",
    "        if(len(ind[key]) != 0):\n",
    "            #print(key, \" -- \", ind[key])\n",
    "            #print(\"===============\")\n",
    "            lis = [0] * (len(set(lab)))\n",
    "            \n",
    "            # open a point in the list\n",
    "            for pt in ind[key]:\n",
    "                \n",
    "                # deal only with points in the required label limit\n",
    "                if pt < lab.shape[0]: \n",
    "                    res = lab[pt]\n",
    "                    lis[res] = lis[res] + 1\n",
    "                    maxi = max(lis)\n",
    "            #print(lis)\n",
    "            \n",
    "            # locate the index of the maximum value and plot only that\n",
    "            for i in range(len(lis)):\n",
    "                if lis[i] == maxi:\n",
    "                    #print(\"MAX: \", maxi)\n",
    "                    pts = i\n",
    "                    #print(\"MAX lab: \", pt)\n",
    "\n",
    "            pts = pts % 40\n",
    "            pts = pts % split\n",
    "            plt.plot(key[0]+0.5, key[1]+0.5, marker='.', markerfacecolor='None', \n",
    "                            markeredgecolor=colorlis[pts], markersize=4, markeredgewidth=1)\n",
    "            \n",
    "            '''if cn % 10000 == 0:\n",
    "                print(\"Done mapping \", cn, \" no. of points\")\n",
    "            cn = cn + 1'''\n",
    "            \n",
    "    \n",
    "    plt.title(thresh_name)\n",
    "    \n",
    "    # save the labeled map\n",
    "    if save == True:\n",
    "        os.chdir(loc)\n",
    "        plt.savefig(thresh_name)\n",
    "        os.chdir('..')\n",
    "        print(\"map saved!!\")\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot the thresholded value based on the maximum count in each cluster\n",
    "def plot_throne(lab, ind, max_size, thresh_name, loc, save):\n",
    "    # lab: 1d labels\n",
    "    # ind: map to list index\n",
    "    # thresh_name: add an extra name\n",
    "    # loc: where to store the results\n",
    "    # save: do you want to save the image (True or False)\n",
    "    \n",
    "    fullset = list(set(lab))\n",
    "    flis = []\n",
    "    \n",
    "    for s in fullset:\n",
    "        plt.axis([0, max_size, 0, max_size])\n",
    "        clis = []\n",
    "        print(\"Current label: \", s)\n",
    "        \n",
    "        # open the map dictionary\n",
    "        for key in ind.keys():\n",
    "\n",
    "            # look into each key\n",
    "            for pt in ind[key]:\n",
    "\n",
    "                lval = lab[pt]\n",
    "                if lval == s:\n",
    "                    plt.plot(key[0]+0.5, key[1]+0.5, marker='.', color='b')\n",
    "                    clis.append(key)\n",
    "                    break\n",
    "\n",
    "        uqlis = list(set(clis))\n",
    "        flis.append(uqlis)\n",
    "        na = thresh_name + str(s)\n",
    "        plt.title(na)\n",
    "\n",
    "        # save the labeled map\n",
    "        if save == True:\n",
    "            os.chdir(loc)\n",
    "            plt.savefig(na)\n",
    "            os.chdir('..')\n",
    "            print(\"map saved!!\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    return flis\n",
    "\n",
    "    \n",
    "# plot the thresholded value based on the maximum count in each cluster\n",
    "def plot_thronev(lab, ind, max_size, thresh_name, loc, save):\n",
    "    # lab: 1d labels\n",
    "    # ind: map to list index\n",
    "    # thresh_name: add an extra name\n",
    "    # loc: where to store the results\n",
    "    # save: do you want to save the image (True or False)\n",
    "    \n",
    "    fidx = []\n",
    "    fullset = list(set(lab))\n",
    "    \n",
    "    for s in fullset:\n",
    "        plt.axis([0, max_size, 0, max_size])\n",
    "        widx = []\n",
    "        # open the map dictionary\n",
    "        print(\"Current label: \", s)\n",
    "        \n",
    "        # open the map dictionary\n",
    "        for key in ind.keys():\n",
    "\n",
    "            # deal only with key having something in the list\n",
    "            if(len(ind[key]) != 0):\n",
    "                #print(key, \" -- \", ind[key])\n",
    "                #print(\"===============\")\n",
    "                lis = [0] * (len(set(lab)))\n",
    "\n",
    "                # open a point in the list\n",
    "                for pt in ind[key]:\n",
    "\n",
    "                    # deal only with points in the required label limit\n",
    "                    if pt < lab.shape[0]: \n",
    "                        res = lab[pt]\n",
    "                        lis[res] = lis[res] + 1\n",
    "                        maxi = max(lis)\n",
    "                #print(lis)\n",
    "\n",
    "                # locate the index of the maximum value and plot only that\n",
    "                for i in range(len(lis)):\n",
    "                    if lis[i] == maxi:\n",
    "                        #print(\"MAX: \", maxi)\n",
    "                        pts = i\n",
    "                        #print(\"MAX lab: \", pt)\n",
    "\n",
    "                if pts == s:\n",
    "                    plt.plot(key[0]+0.5, key[1]+0.5, marker='.', color='b')\n",
    "                    widx.append(key)\n",
    "                    \n",
    "                #else:\n",
    "                    #plt.plot(key[0]+0.5, key[1]+0.5, marker='.', color='y')\n",
    "\n",
    "\n",
    "        na = thresh_name + str(s)\n",
    "        plt.title(na)\n",
    "\n",
    "        # save the labeled map\n",
    "        if save == True:\n",
    "            os.chdir(loc)\n",
    "            plt.savefig(na)\n",
    "            os.chdir('..')\n",
    "            print(\"map saved!!\")\n",
    "\n",
    "        plt.show()\n",
    "        fidx.append(widx)\n",
    "        \n",
    "    return fidx\n",
    "\n",
    "\n",
    "# function to locate junctions between label classes\n",
    "def ret_junc(label):\n",
    "    junc = []\n",
    "    junc.append(0)\n",
    "    for i in range(1, label.shape[0]):\n",
    "        if label[i - 1] != label[i]:\n",
    "            junc.append(i)\n",
    "    junc.append(label.shape[0]-1)\n",
    "    print(junc)\n",
    "    return junc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more thresholding functions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Apply the Mapping..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Thresh or Not to Thresh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to work with more than one label after getting indices\n",
    "def lab_thrline(label, index, size, name, loc, save):\n",
    "    \n",
    "    t_name = name + \"_thrline\"\n",
    "    plot_thrline(label, index, size, t_name, loc, save)\n",
    "    return\n",
    "\n",
    "\n",
    "# function to work with more than one label after getting indices\n",
    "def lab_thrcolor(label, index, size, name, loc, save):\n",
    "    \n",
    "    t_name = name + \"_thrcolor\"\n",
    "    plot_thrcolor(label, index, size, t_name, loc, save)\n",
    "    return\n",
    "\n",
    "\n",
    "# function to work with more than one label after getting indices\n",
    "def lab_thresh(label, index, size, name, loc, save):\n",
    "    \n",
    "    t_name = name + \"_thr\"\n",
    "    plot_thresh(label, index, size, t_name, loc, save)\n",
    "    return\n",
    " \n",
    "\n",
    "# function to work with a single label over only one driver dataset\n",
    "def lab_throne(label, index, size, name, loc, save):\n",
    "    \n",
    "    t_name = name + \"_thr\"\n",
    "    wind = plot_throne(label, index, size, name, loc, save)\n",
    "    return wind\n",
    "\n",
    "\n",
    "# function to work with a single label over only one driver dataset\n",
    "def lab_thronev(label, index, size, name, loc, save):\n",
    "    \n",
    "    t_name = name + \"_thrvs\"\n",
    "    wind = plot_thronev(label, index, size, name, loc, save)\n",
    "    return wind\n",
    "\n",
    "\n",
    "## MOST LIKELY WILL NOT BE USED?!\n",
    "# function that will do the labeling by-passing the thresholding function\n",
    "def lab_nothresh(label, index, size, name, loc, save):\n",
    "    \n",
    "    # set the markers and colors\n",
    "    marker = ['o', '*', 's', 'D', '^', '+']\n",
    "    color = ['r', 'g', 'b', 'y', 'c', 'k']\n",
    "    \n",
    "    cn = 0\n",
    "    \n",
    "    plt.axis([0, size, 0, size])\n",
    "    print(\"Beginning plotting - no threshold for \", name, '...')\n",
    "    # open the index\n",
    "    for key in index.keys():\n",
    "        \n",
    "        # get list of values per key\n",
    "        li = index[key]\n",
    "        la = []\n",
    "        for pt in li:\n",
    "            lab = label[pt]\n",
    "            la.append(lab)\n",
    "            \n",
    "            if cn%10000 == 0:\n",
    "                print(\"done with \", cn, \" number of points...\")\n",
    "            cn = cn + 1\n",
    "            \n",
    "        stl = set(la)\n",
    "        for ss in stl:\n",
    "            plt.plot(key[0]+0.5, key[1]+0.5, marker[ss], markerfacecolor='None', \n",
    "                     markeredgecolor=color[ss], markersize=4, markeredgewidth=1)\n",
    "            \n",
    "            \n",
    "    print(\"Plotting of \", name, ' done...')\n",
    "    new_n = name + '_nothr'\n",
    "    plt.title(new_n)\n",
    "    \n",
    "    if save == True:\n",
    "        os.chdir(loc)\n",
    "        plt.savefig(new_n)\n",
    "        os.chdir('..')\n",
    "        print(\"color map saved!!\")\n",
    "        \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different variations of starter functions to SOMs..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial runners..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NOTE: all functions below apply for only one type of label at a time (unless future modifications are made...)\n",
    "\n",
    "# function to performs single data - single label - single weights labeling\n",
    "## used as a test function in case of unknown errors\n",
    "def onlyone_1lab(data, label, weight, loc, name, save):\n",
    "    \n",
    "    win, idx = slide_lab(data, weight, loc)\n",
    "    lab_thresh(label, idx, weight.shape[0], loc, name, save)\n",
    "    \n",
    "    # create and return dictionary of mapped indices\n",
    "    dict_idx = join_dict(wt_k, idx_li)\n",
    "    return dict_idx\n",
    "\n",
    "\n",
    "# function to perform single data - single label - multi weights labeling\n",
    "## kind of a test function for SOM parameters\n",
    "def single_1lab(data, label, lab_name, weight_dict, loc, thresh, save):\n",
    "    \n",
    "    wt_k, wt_v = sep_dict(weight_dict)\n",
    "    \n",
    "    idx_li = []\n",
    "    \n",
    "    # set the weights first\n",
    "    for wt in range(len(wt_k)):\n",
    "        \n",
    "        save_n = wt_k[wt].rstrip('.npy')\n",
    "        \n",
    "        print(\"Starting \", wt_k[wt], ' ...')\n",
    "        # keys are not needed\n",
    "        cur_wt = wt_v[wt]\n",
    "            \n",
    "        # get the mapping\n",
    "        win, idx = slide_lab(data, cur_wt, save_n, loc, save)\n",
    "        idx_li.append(idx)\n",
    "\n",
    "        # new name of the map\n",
    "        name = \"plt_\" + lab_name\n",
    "\n",
    "        # plot the mapping\n",
    "        if thresh == True:\n",
    "            lab_thresh(label, idx, cur_wt.shape[0], save_n, loc, save)\n",
    "        else:\n",
    "            lab_nothresh(label, idx, cur_wt.shape[0], save_n, loc, save)\n",
    "            \n",
    "    # create and return dictionary of mapped indices\n",
    "    dict_idx = join_dict(wt_k, idx_li)\n",
    "    return dict_idx\n",
    "\n",
    "# function to modify all involved dictionaries\n",
    "def mod_dict(da_d, la_d, wt_d, id_d, excepts):\n",
    "    \n",
    "    da_k, da_v = sep_dict(da_d)\n",
    "    la_k, la_v = sep_dict(la_d)\n",
    "    wt_k, wt_v = sep_dict(wt_d)\n",
    "    id_k, id_v = sep_dict(id_d)\n",
    "    \n",
    "    da_m, la_m, wt_m, id_m = dict(), dict(), dict(), dict()\n",
    "    f0 = 0\n",
    "    \n",
    "    for i in range(len(la_k)):\n",
    "        for j in range(len(excepts)):\n",
    "            if la_k[i] != excepts[j]:\n",
    "                f0 = 0\n",
    "            else:\n",
    "                f0 = 1\n",
    "        \n",
    "        if f0 == 0:\n",
    "            da_m[da_k[i]] = da_v[i]\n",
    "            la_m[la_k[i]] = la_v[i]\n",
    "            wt_m[wt_k[i]] = wt_v[i]\n",
    "            id_m[id_k[i]] = id_v[i]\n",
    "            \n",
    "    return da_m, la_m, wt_m, id_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import sys\n",
    "!{sys.executable} -m pip install '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All runners..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform multi data - multi label - multi weight labeling\n",
    "def all_1lab(data_dict, label_dict, weight_dict, loc, thresh, save):\n",
    "    \n",
    "    wt_k, wt_v = lookup(weight_dict)\n",
    "    da_k, da_v = lookup(data_dict)\n",
    "    la_k, la_v = lookup(label_dict)\n",
    "    \n",
    "    id_k, id_v = [], []\n",
    "    \n",
    "    # set the weights first\n",
    "    for wt in range(len(wt_k)):\n",
    "        \n",
    "        save_n = wt_k[wt].rstrip('.npy')\n",
    "        \n",
    "        print(\"Starting \", wt_k[wt], ' ...')\n",
    "        # keys are not needed\n",
    "        \n",
    "        # setting values for the current iteration\n",
    "        cur_wt = wt_v[wt]\n",
    "        cur_da = da_v[wt]\n",
    "        cur_la = la_v[wt]\n",
    "        \n",
    "        # get the mapping\n",
    "        win, idx = slide_lab(cur_da, cur_wt, save_n, loc, save)\n",
    "        id_v.append(idx)\n",
    "        id_k.append(save_n)\n",
    "        \n",
    "        # new name of the map\n",
    "        name = \"plt_\" + save_n\n",
    "\n",
    "        # plot the mapping\n",
    "        if thresh == True:\n",
    "            lab_thresh(cur_la, idx, cur_wt.shape[0], name, loc, save)\n",
    "        else:\n",
    "            lab_nothresh(cur_la, idx, cur_wt.shape[0], name, loc, save)\n",
    "            \n",
    "    print(\"Mapping Complete for \", save_n)\n",
    "    # create and return dictionary of mapped indices\n",
    "    dict_idx = join_dict(id_k, id_v)\n",
    "    return dict_idx\n",
    "\n",
    "\n",
    "## NOTE: only after we get the mappings of the SOM can we label them\n",
    "\n",
    "# function to perform color scale labeling without the need for mapping\n",
    "def all_1lab_thrline(label_dict, weight_dict, idx_dict, loc, ext, save):\n",
    "    \n",
    "    wt_k, wt_v = lookup(weight_dict)\n",
    "    id_k, id_v = lookup(idx_dict)\n",
    "    la_k, la_v = lookup(label_dict)\n",
    "    \n",
    "    # set the weights first\n",
    "    for wtid in range(len(wt_k)):\n",
    "        \n",
    "        save_n = wt_k[wtid].rstrip('.npy')\n",
    "        \n",
    "        print(\"Starting \", wt_k[wtid], ' ...')\n",
    "        # keys are not needed\n",
    "        \n",
    "        # setting values for the current iteration\n",
    "        cur_wt = wt_v[wtid]\n",
    "        cur_id = id_v[wtid]\n",
    "        cur_la = la_v[wtid]\n",
    "        \n",
    "        # new name of the map\n",
    "        name = \"plt_\" + save_n + ext\n",
    "\n",
    "        # plot the mapping\n",
    "        lab_thrline(cur_la, cur_id, cur_wt.shape[0], name, loc, save) \n",
    "            \n",
    "        print(\"Plotting done!!\\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "# function to perform multi color labeling without the need for mapping\n",
    "def all_1lab_thrcolor(label_dict, weight_dict, idx_dict, loc, ext, save):\n",
    "    \n",
    "    wt_k, wt_v = lookup(weight_dict)\n",
    "    id_k, id_v = lookup(idx_dict)\n",
    "    la_k, la_v = lookup(label_dict)\n",
    "    \n",
    "    # set the weights first\n",
    "    for wtid in range(len(wt_k)):\n",
    "        \n",
    "        save_n = wt_k[wtid].rstrip('.npy')\n",
    "        \n",
    "        print(\"Starting \", wt_k[wtid], ' ...')\n",
    "        # keys are not needed\n",
    "        \n",
    "        # setting values for the current iteration\n",
    "        cur_wt = wt_v[wtid]\n",
    "        cur_id = id_v[wtid]\n",
    "        cur_la = la_v[wtid]\n",
    "        \n",
    "        # new name of the map\n",
    "        name = \"plt_\" + save_n + ext\n",
    "\n",
    "        # plot the mapping\n",
    "        lab_thrcolor(cur_la, cur_id, cur_wt.shape[0], name, loc, save) \n",
    "            \n",
    "        print(\"Plotting done!!\\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# function to perform multi color labeling without the need for mapping\n",
    "def all_1lab_throne(wtid, label_dict, weight_dict, idx_dict, loc, ext, save):\n",
    "    \n",
    "    wt_k, wt_v = lookup(weight_dict)\n",
    "    id_k, id_v = lookup(idx_dict)\n",
    "    la_k, la_v = lookup(label_dict)\n",
    "    \n",
    "        \n",
    "    save_n = wt_k[wtid].rstrip('.npy')\n",
    "\n",
    "    print(\"Starting \", wt_k[wtid], ' ...')\n",
    "    # keys are not needed\n",
    "\n",
    "    # setting values for the current iteration\n",
    "    cur_wt = wt_v[wtid]\n",
    "    cur_id = id_v[wtid]\n",
    "    cur_la = la_v[wtid]\n",
    "\n",
    "    # new name of the map\n",
    "    name = \"plt_\" + save_n + ext\n",
    "\n",
    "    # plot the mapping\n",
    "    thewins = lab_throne(cur_la, cur_id, cur_wt.shape[0], name, loc, save) \n",
    "\n",
    "    print(\"Plotting done!!\\n\")\n",
    "\n",
    "    return thewins\n",
    "\n",
    "\n",
    "# function to perform one vs all mapping with thresholding\n",
    "def all_1lab_thronev(thisid, label_dict, weight_dict, idx_dict, loc, ext, save):\n",
    "    \n",
    "    wt_k, wt_v = lookup(weight_dict)\n",
    "    id_k, id_v = lookup(idx_dict)\n",
    "    la_k, la_v = lookup(label_dict)\n",
    "    wtid = thisid\n",
    "    \n",
    "        \n",
    "    save_n = wt_k[wtid].rstrip('.npy')\n",
    "\n",
    "    print(\"Starting \", wt_k[wtid], ' ...')\n",
    "    # keys are not needed\n",
    "\n",
    "    # setting values for the current iteration\n",
    "    cur_wt = wt_v[wtid]\n",
    "    cur_id = id_v[wtid]\n",
    "    cur_la = la_v[wtid]\n",
    "\n",
    "    # new name of the map\n",
    "    name = \"plt_\" + save_n + ext\n",
    "\n",
    "    # plot the mapping\n",
    "    onev = lab_thronev(cur_la, cur_id, cur_wt.shape[0], name, loc, save) \n",
    "\n",
    "    print(\"Plotting done!!\\n\")\n",
    "\n",
    "    return onev\n",
    "\n",
    "\n",
    "# function to run all while circumventing the need to perfom SOM mapping and even creating a SOM object\n",
    "def all_1lab_noobj(label_dict, weight_dict, idx_dict, loc, ext, thresh, save):\n",
    "    \n",
    "    wt_k, wt_v = lookup(weight_dict)\n",
    "    la_k, la_v = lookup(label_dict)\n",
    "    id_k, id_v = lookup(idx_dict)\n",
    "    \n",
    "    # set the weights first\n",
    "    for wtid in range(len(wt_k)):\n",
    "        \n",
    "        save_n = wt_k[wtid].rstrip('.npy')\n",
    "        \n",
    "        print(\"Starting \", wt_k[wtid], ' ...')\n",
    "        # keys are not needed\n",
    "        \n",
    "        # setting values for the current iteration\n",
    "        cur_wt = wt_v[wtid]\n",
    "        cur_la = la_v[wtid]\n",
    "        cur_id = id_v[wtid]\n",
    "        \n",
    "        # already got the SOM map\n",
    "\n",
    "        # new name of the map\n",
    "        name = \"plt_\" + save_n + ext\n",
    "\n",
    "        # plot the mapping\n",
    "        if thresh == True:\n",
    "            lab_thresh(cur_la, cur_id, cur_wt.shape[0], name, loc, save)\n",
    "        else:\n",
    "            lab_nothresh(cur_la, cur_id, cur_wt.shape[0], name, loc, save)\n",
    "            \n",
    "        print(\"Plotting done!!\\n\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the SOM labeling functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find max in a mapping\n",
    "def maxmap(map_v):\n",
    "    maxi = 0\n",
    "    \n",
    "    for kk in map_v.keys():\n",
    "        li = map_v[kk]\n",
    "        for i in li:\n",
    "            if i > maxi:\n",
    "                maxi = i\n",
    "                tk = kk\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    print(\"max: \", maxi)\n",
    "    print(\"maxk: \", tk)\n",
    "    return\n",
    "\n",
    "# function to display map\n",
    "def dispmap(map_v):\n",
    "    \n",
    "    for kk in map_v.keys():\n",
    "        print(\"Key: \", kk)\n",
    "        print(\"Val: \", map_v[kk])\n",
    "        print(\"**********************************\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Brake Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxmap(tr_map['ten_re_id.csv'])\n",
    "br_remap['te_brl'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the brake label but with mapping already done\n",
    "all_1lab_noobj(br_remap, tr_wt, tr_map, \"results\", \"_brn\", True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Speed Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the speed label but with mapping already done\n",
    "all_1lab_noobj(sd_remap, tr_wt, tr_map, \"results\", \"_sdl\", True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Forward Backward Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the forward backward label but with mapping already done\n",
    "all_1lab_noobj(fb_remap, tr_wt, tr_map, \"results\", \"_fbl\", True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Left Right Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the left right label but with mapping already done\n",
    "all_1lab_noobj(lr_remap, tr_wt, tr_map, \"results\", \"_lrl\", True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Top Bottom Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the top bottom label but with mapping already done\n",
    "all_1lab_noobj(tb_remap, tr_wt, tr_map, \"results\", \"_tbl\", True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Lane Gap Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the lane gap label but with mapping already done\n",
    "all_1lab_noobj(lg_remap, tr_wt, tr_map, \"results\", \"_lgl\", True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying distract label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di_di, tr_di, di_map = dict(), dict(), dict()\n",
    "di_di['distract'] = di_remap\n",
    "tr_di['aln_re_wt.npy'] = tr_wt['aln_re_wt.npy']\n",
    "di_map['aln_re_id.csv'] = tr_map['aln_re_id.csv']\n",
    "all_1lab_noobj(di_di, tr_di, di_map, \"results\", \"_dil\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying 3 Segment Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the segment 3 label but with mapping already done\n",
    "all_1lab_noobj(s3_remap, tr_wt, tr_map, \"results\", \"_s3n\", True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying 2 segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the segment 2 label but with mapping already done\n",
    "all_1lab_noobj(s2_remap, tr_wt, tr_map, \"results\", \"_s2l\", True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying 4 segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the segment 4 label but with mapping already done\n",
    "all_1lab_noobj(s4_remap, tr_wt, tr_map, \"results\", \"_s4l\", True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying 5 segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the segment 5 label but with mapping already done\n",
    "all_1lab_noobj(s5_remap, tr_wt, tr_map, \"results\", \"_s5l\", True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying black-gray scale labeling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# black to gray scale label over the map\n",
    "all_1lab_thrline(s2_remap, tr_wt, tr_map, \"results\", \"_btg\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# black to gray scale label over the map\n",
    "all_1lab_thrline(s3_remap, tr_wt, tr_map, \"results\", \"_b3g\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# black to gray scale label over the map\n",
    "all_1lab_thrline(s4_remap, tr_wt, tr_map, \"results\", \"_b4g\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# black to gray scale label over the map\n",
    "all_1lab_thrline(s5_remap, tr_wt, tr_map, \"results\", \"_b5g\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# black to gray scale label over the map\n",
    "all_1lab_thrline(s6_remap, tr_wt, tr_map, \"results\", \"_b6g\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# black to gray scale label over the map\n",
    "all_1lab_thrline(s7_remap, tr_wt, tr_map, \"results\", \"_b7g\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_di = dict()\n",
    "for key in sp_remap.keys():\n",
    "    alist = np.zeros(sp_remap[key].shape, dtype='int')\n",
    "    for i in range(len(sp_remap[key])):\n",
    "        alist[i] = i\n",
    "    li_di[key] = alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# black to gray scale label over the map\n",
    "all_1lab_thrline(li_di, tr_wt, tr_map, \"results\", \"_bg\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using multi colors..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Split Label.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the multi color split to 3 distractions but with mapping already done\n",
    "all_1lab_thrcolor(sp_remap, tr_wt, tr_map, \"results\", \"_spl\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the multi color split to all distractions label but with mapping already done\n",
    "all_1lab_thrcolor(sp_remap, tr_wt, tr_map, \"results\", \"_spl\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakup and label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 divisions\n",
    "# applying break up label but with mapping already done\n",
    "all_1lab_thrcolor(sp_remap, tr_wt, tr_map, \"results\", \"_sp2set\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 divisions\n",
    "# applying break up label but with mapping already done\n",
    "all_1lab_thrcolor(sp_remap, tr_wt, tr_map, \"results\", \"_sp3set\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4 divisions\n",
    "# applying break up label but with mapping already done\n",
    "all_1lab_thrcolor(sp_remap, tr_wt, tr_map, \"results\", \"_sp4set\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 5 divisions\n",
    "# applying break up label but with mapping already done\n",
    "all_1lab_thrcolor(sp_remap, tr_wt, tr_map, \"results\", \"_sp5set\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8 divisions\n",
    "# applying break up label but with mapping already done\n",
    "all_1lab_thrcolor(sp_remap, tr_wt, tr_map, \"results\", \"_sp8set\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 10 divisions\n",
    "# applying break up label but with mapping already done\n",
    "all_1lab_thrcolor(sp_remap, tr_wt, tr_map, \"results\", \"_sp10set\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One driver at a time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one hf label only over the whole map\n",
    "allhf = all_1lab_throne(1, sp_remap, tr_wt, tr_map, \"results\", \"_one\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one mu label only over the whole map\n",
    "allmu = all_1lab_throne(2, sp_remap, tr_wt, tr_map, \"results\", \"_one\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one te label only over the whole map\n",
    "allte = all_1lab_throne(3, sp_remap, tr_wt, tr_map, \"results\", \"_one\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one al label only over the whole map\n",
    "allal = all_1lab_throne(0, sp_remap, tr_wt, tr_map, \"results\", \"_one\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing one vs all drivers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one vs all hf labels\n",
    "indhf = all_1lab_thronev(1, sp_remap, tr_wt, tr_map, \"results\", \"_vs\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one vs all mus labels\n",
    "indmu = all_1lab_thronev(2, sp_remap, tr_wt, tr_map, \"results\", \"_vs\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one vs all tex labels\n",
    "indte = all_1lab_thronev(3, sp_remap, tr_wt, tr_map, \"results\", \"_vs\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one vs all al labels\n",
    "indal = all_1lab_thronev(0, sp_remap, tr_wt, tr_map, \"results\", \"_vs\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to understand mappings\n",
    "# function to set the mapping values to keys\n",
    "def set_map(labeler, listofkeys):\n",
    "    \n",
    "    fin = defaultdict(list)\n",
    "    for key in listofkeys:\n",
    "        k1 = key[0]\n",
    "        k2 = key[1]\n",
    "        klist = labeler[k1, k2]\n",
    "        \n",
    "        for kk in klist:\n",
    "            fin[k1, k2].append(kk)\n",
    "        \n",
    "    return fin\n",
    "        \n",
    "\n",
    "# save the mappings obtained...\n",
    "# store the index as a csv file\n",
    "def store_map(fname, cur_dict):\n",
    "    \n",
    "    os.chdir('mapping')\n",
    "    name = fname + '.csv'\n",
    "    with open(name, 'w') as f:\n",
    "        \n",
    "        for key in cur_dict.keys():\n",
    "            cur_lis = cur_dict[key]\n",
    "            cur_len = len(cur_lis)\n",
    "            \n",
    "            f.write(\"%d,%d,%d,\"%(key[0], key[1], cur_len))\n",
    "            \n",
    "            for li in cur_lis:\n",
    "                f.write(\"%d,\"%(li))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "        print(\"File \", name, \" saved!!\")\n",
    "        \n",
    "    os.chdir('..')\n",
    "    return\n",
    "\n",
    "# load up the index from a csv file\n",
    "def load_map(fname):\n",
    "    \n",
    "    os.chdir('mapping')\n",
    "    idx = defaultdict(list)\n",
    "    \n",
    "    with open(fname, 'rt') as f:\n",
    "        cur_csv = csv.reader(f)\n",
    "        \n",
    "        for line in cur_csv:\n",
    "            for val in range(3, int(line[2])+3):\n",
    "                \n",
    "                k1 = int(line[0])\n",
    "                k2 = int(line[1])\n",
    "                vv = int(line[val])\n",
    "                \n",
    "                idx[k1, k2].append(vv)\n",
    "                \n",
    "    os.chdir('..')\n",
    "    print(\"File \", fname, \" has been loaded\")\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the mappings of each coordinate [one vs all]\n",
    "\n",
    "hfq = dict()\n",
    "i = 0\n",
    "for hfmap in indhf:\n",
    "    namehf = \"hfq_map\" + str(i)\n",
    "    nmap = set_map(tr_map['hfn_re_id.csv'], hfmap)\n",
    "    hfq[namehf] = nmap\n",
    "    i = i + 1\n",
    "    \n",
    "\n",
    "muq = dict()\n",
    "j = 0\n",
    "for mumap in indmu:\n",
    "    namemu = \"muq_map\" + str(j)\n",
    "    nmap = set_map(tr_map['mun_re_id.csv'], mumap)\n",
    "    muq[namemu] = nmap\n",
    "    j = j + 1\n",
    "\n",
    "    \n",
    "teq = dict()\n",
    "k = 0\n",
    "for temap in indte:\n",
    "    namete = \"teq_map\" + str(k)\n",
    "    nmap = set_map(tr_map['ten_re_id.csv'], temap)\n",
    "    teq[namete] = nmap\n",
    "    k = k + 1\n",
    "    \n",
    "    \n",
    "alq = dict()\n",
    "l = 0\n",
    "for almap in indal:\n",
    "    nameal = \"alq_map\" + str(l)\n",
    "    nmap = set_map(tr_map['aln_re_id.csv'], almap)\n",
    "    alq[nameal] = nmap\n",
    "    l = l + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the mappings [one vs all]\n",
    "\n",
    "for hf in hfq.keys():\n",
    "    store_map(hf, hfq[hf])\n",
    "    \n",
    "for mu in muq.keys():\n",
    "    store_map(mu, muq[mu])\n",
    "    \n",
    "for te in teq.keys():\n",
    "    store_map(te, teq[te])\n",
    "    \n",
    "for al in alq.keys():\n",
    "    store_map(al, alq[al])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating the mappings of each coordinate [one only]\n",
    "\n",
    "hfa = dict()\n",
    "i = 0\n",
    "for hfmap in allhf:\n",
    "    namehf = \"hfa_map\" + str(i)\n",
    "    nmap = set_map(tr_map['hfn_re_id.csv'], hfmap)\n",
    "    hfa[namehf] = nmap\n",
    "    i = i + 1\n",
    "    \n",
    "\n",
    "mua = dict()\n",
    "j = 0\n",
    "for mumap in allmu:\n",
    "    namemu = \"mua_map\" + str(j)\n",
    "    nmap = set_map(tr_map['mun_re_id.csv'], mumap)\n",
    "    mua[namemu] = nmap\n",
    "    j = j + 1\n",
    "\n",
    "    \n",
    "tea = dict()\n",
    "k = 0\n",
    "for temap in allte:\n",
    "    namete = \"tea_map\" + str(k)\n",
    "    nmap = set_map(tr_map['ten_re_id.csv'], temap)\n",
    "    tea[namete] = nmap\n",
    "    k = k + 1\n",
    "    \n",
    "    \n",
    "ala = dict()\n",
    "l = 0\n",
    "for almap in allal:\n",
    "    nameal = \"ala_map\" + str(l)\n",
    "    nmap = set_map(tr_map['aln_re_id.csv'], almap)\n",
    "    ala[nameal] = nmap\n",
    "    l = l + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the mappings [one only]\n",
    "\n",
    "for hf in hfa.keys():\n",
    "    store_map(hf, hfa[hf])\n",
    "    \n",
    "for mu in mua.keys():\n",
    "    store_map(mu, mua[mu])\n",
    "    \n",
    "for te in tea.keys():\n",
    "    store_map(te, tea[te])\n",
    "    \n",
    "for al in ala.keys():\n",
    "    store_map(al, ala[al])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ala_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "ala_map = load_idx(ala_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hfa_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "hfa_map = load_idx(hfa_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mua_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "mua_map = load_idx(mua_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tea_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "tea_map = load_idx(tea_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "alq_map = load_idx(alq_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hfq_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "hfq_map = load_idx(hfq_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "muq_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "muq_map = load_idx(muq_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "teq_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "teq_map = load_idx(teq_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to connect the dots\n",
    "def plot_pts(mapper, size):\n",
    "    for the in mapper.keys():\n",
    "        plt.axis([0, size, 0, size])\n",
    "        theset = mapper[the]\n",
    "        k1, k2 = [], []\n",
    "        print(\"Mapper: \", the)\n",
    "        \n",
    "        totlen = len(theset)\n",
    "        bgp = np.linspace(0, 0.85, tot_lab)\n",
    "        itr = 0\n",
    "        \n",
    "        for ky in theset.keys():\n",
    "            #print(ky)\n",
    "            k1.append(ky[0]+0.5)\n",
    "            k2.append(ky[1]+0.5)\n",
    "            plt.plot(ky[0]+0.5, ky[1]+0.5, color=str(bgs[itr]), marker='s')\n",
    "\n",
    "        plt.plot(k1, k2, 'y--')\n",
    "        plt.show()\n",
    "        \n",
    "    for the in mapper.keys():\n",
    "        print(\"Pts size: \", len(mapper[the]))\n",
    "\n",
    "\n",
    "# label to understand the timings\n",
    "def plot_time(mapper, size, split):\n",
    "    \n",
    "    for the in mapper.keys():\n",
    "        plt.axis([0, size, 0, size])\n",
    "        theset = mapper[the]\n",
    "        k1, k2 = [], []\n",
    "        print(\"Mapper: \", the)\n",
    "        tot_lab = len(theset)\n",
    "        colorlis = np.linspace(0, 0.85, tot_lab)\n",
    "        itr = 0\n",
    "        \n",
    "        for ky in theset.keys():\n",
    "            k1.append(ky[0]+0.5)\n",
    "            k2.append(ky[1]+0.5)\n",
    "            plt.plot(ky[0]+0.5, ky[1]+0.5, color=str(colorlis[itr]), marker='o')\n",
    "            itr = itr + 1\n",
    "\n",
    "        #plt.plot(k1, k2, 'y--')\n",
    "        plt.show()\n",
    "        \n",
    "    for the in mapper.keys():\n",
    "        print(\"Pts size: \", the, len(mapper[the]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pts(alq_map, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pts(hfq_map, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pts(muq_map, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pts(teq_map, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time(hfq_map, 60, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time(hfa_map, 60, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to understand timings\n",
    "for hf in hfq_map.keys():\n",
    "    plt.axis([0, 60, 0, 60])\n",
    "    theset = hfq_map[hf]\n",
    "    k1, k2 = [], []\n",
    "    \n",
    "    for ky in theset.keys():\n",
    "        #print(ky)\n",
    "        k1.append(ky[0]+0.5)\n",
    "        k2.append(ky[1]+0.5)\n",
    "        plt.plot(ky[0]+0.5, ky[1]+0.5, 'bs')\n",
    "        \n",
    "    plt.plot(k1, k2, 'r--')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the demo labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.read_excel('demo.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual = pd.DataFrame(demo.iloc[:1, :])\n",
    "\n",
    "for i in range(demo.shape[0]):\n",
    "    if demo.iloc[i, 1] == 1:\n",
    "        app = pd.DataFrame(demo.iloc[i:i+1, :])\n",
    "        dual = dual.append(app, ignore_index=True)\n",
    "        \n",
    "fi = dual.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining the demo file...\n",
    "li_lab = fi.columns.values\n",
    "\n",
    "for i in range(len(li_lab)):\n",
    "    print(i, \"---\")\n",
    "    print(fi[li_lab[i]].value_counts())\n",
    "    print(\"*******************************************\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# arrangement of labels\n",
    "def look_lab():\n",
    "    for i in range(li_lab.shape[0]):\n",
    "        print(i, \" ----> \", li_lab[i])\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "look_lab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizing the labels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a label from demo file\n",
    "# modifying the existing split label by applying the new demo labels\n",
    "\n",
    "def get_demo(lsplit, colu):\n",
    "    dt = type(fi[colu].iloc[0])\n",
    "    nlab = np.zeros(fi.shape[0], dtype=dt).reshape(-1, 1)\n",
    "    for i in range(fi.shape[0]):\n",
    "        nlab[i] = fi[colu].iloc[i]\n",
    "        \n",
    "    slab = set(nlab[:, 0])\n",
    "    print(slab)\n",
    "    dlab = dict()\n",
    "    itr = 0\n",
    "    for s in slab:\n",
    "        dlab[s] = itr\n",
    "        itr = itr + 1\n",
    "    print(dlab)\n",
    "    \n",
    "    fin = (nlab, dlab)\n",
    "    sp_k, sp_v = lookup(lsplit)\n",
    "    nl, dl = fin[0], fin[1]\n",
    "    new_l = []\n",
    "    \n",
    "    for i in range(len(sp_k)):\n",
    "        cur_k = sp_k[i]\n",
    "        cur_v = sp_v[i]\n",
    "        \n",
    "        new_lab = np.zeros(cur_v.shape, dtype='int').reshape(-1, 1)\n",
    "        \n",
    "        for i in range(new_lab.shape[0]):\n",
    "            cur_sp = cur_v[i] # get the index of the demolab\n",
    "            if cur_sp >= 40:\n",
    "                cur_sp = cur_sp % 40\n",
    "            sp_nl = nl[cur_sp, 0] # get the exact demolab based on index [exact label]\n",
    "            nl_dl = dl[sp_nl] # get the set dictionary value to map [exact mapping]\n",
    "            new_lab[i, 0] = nl_dl # assignment [add to final label]\n",
    "            \n",
    "        new_l.append(new_lab[:, 0])\n",
    "    \n",
    "    new_di = join_dict(sp_k, new_l)\n",
    "    print(\"The Mapping: \", dl)\n",
    "    return new_di"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen2 = get_demo(sp_remap, \"Intake_Gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the gender intake label but with mapping already done\n",
    "all_1lab_noobj(gen2, tr_wt, tr_map, \"results\", \"_gen2\", True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age3 = get_demo(sp_remap, \"Intake_Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# applying the segment 5 label but with mapping already done\n",
    "all_1lab_noobj(age3, tr_wt, tr_map, \"results\", \"_age3\", True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye Problem Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye4 = get_demo(sp_remap, \"Intake_ProbEyes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying clustering methods..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting individual sets of points\n",
    "def plotter(plotset, size):\n",
    "    plt.axis([0, size, 0, size])\n",
    "    for key in plotset:\n",
    "        plt.plot(key[0], key[1], 'b.')\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "# converting keys to xy axis\n",
    "def convkeys(keyset):\n",
    "    conv = np.zeros(len(keyset)*2).reshape(-1, 2)\n",
    "    \n",
    "    i = 0\n",
    "    j = 0\n",
    "    for ak in keyset:\n",
    "        conv[i, j] = ak[0]\n",
    "        conv[i, j+1] = ak[1]\n",
    "        i = i + 1\n",
    "        \n",
    "    return conv\n",
    "\n",
    "# apply and display kmeans, display original\n",
    "def dispkmeans(kmap, imap, size):\n",
    "    # getting the keys\n",
    "    ktest = list(kmap.keys())\n",
    "    itest = list(imap.keys())\n",
    "    kconv = convkeys(ktest)\n",
    "    iconv = convkeys(itest)\n",
    "    \n",
    "    # plotting the all map\n",
    "    print(\"Plotting the all points map before kmeans clustering...\")\n",
    "    plotter(kmap, size)\n",
    "    \n",
    "    # creating the kmeans model\n",
    "    ktot = len(itest)\n",
    "    kmodel = KMeans(n_clusters=ktot)\n",
    "    kmodel.fit(kconv)\n",
    "    kclust = kmodel.cluster_centers_\n",
    "    \n",
    "    # plotting the kmeans cluster centers\n",
    "    print(\"Total keys in map after kmeans: \", ktot, len(itest))\n",
    "    print(\"Keys resulting from the Kmeans centers: \", kclust)\n",
    "    plt.axis([0, size, 0, size])\n",
    "    plt.scatter(kclust[:, 0], kclust[:, 1], label='True Position')\n",
    "    print(\"Plotting the resulting Kmeans centers map...\")\n",
    "    plt.show()\n",
    "    \n",
    "    # plotting the unique SOM results\n",
    "    print(\"Unique keys resulting from the SOM: \", itest)\n",
    "    plt.axis([0, size, 0, size])\n",
    "    plt.scatter(iconv[:, 0], iconv[:, 1], label='True Position')\n",
    "    print(\"Plotting the unique map clusters...\")\n",
    "    plt.show()\n",
    "    print(\"****************************************\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k1, k2 in zip(hfa_map.keys(), hfq_map.keys()):\n",
    "    print(k1, k2)\n",
    "    dispkmeans(hfa_map[k1], hfq_map[k2], 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k1, k2 in zip(mua_map.keys(), muq_map.keys()):\n",
    "    print(k1, k2)\n",
    "    dispkmeans(mua_map[k1], muq_map[k2], 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k1, k2 in zip(tea_map.keys(), teq_map.keys()):\n",
    "    print(k1, k2)\n",
    "    dispkmeans(tea_map[k1], teq_map[k2], 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k1, k2 in zip(ala_map.keys(), alq_map.keys()):\n",
    "    print(k1, k2)\n",
    "    dispkmeans(ala_map[k1], alq_map[k2], 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Hierarchial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hierarchical clustering libraries\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kval = list(hfa_map['hfa_map0.csv'].keys())\n",
    "points = convkeys(kval)\n",
    "upts = list(hfq_map['hfq_map0.csv'].keys())\n",
    "# create dendrogram\n",
    "dendrogram = sch.dendrogram(sch.linkage(points, method='ward'))\n",
    "# create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=len(upts), affinity = 'euclidean', linkage = 'ward')\n",
    "# save clusters for chart\n",
    "y_hc = hc.fit_predict(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upts = list(hfq_map['hfq_map0.csv'].keys())\n",
    "cpts = convkeys(upts)\n",
    "# create dendrogram\n",
    "dendrogram = sch.dendrogram(sch.linkage(points, method='ward'))\n",
    "# create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=len(upts), affinity = 'euclidean', linkage = 'ward')\n",
    "# save clusters for chart\n",
    "y_hc = hc.fit_predict(upts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dendrogram\n",
    "dendrogram = sch.dendrogram(sch.linkage(cpts, method='ward'))\n",
    "# create clusters\n",
    "hc = AgglomerativeClustering(n_clusters=len(upts), affinity = 'euclidean', linkage = 'ward')\n",
    "# save clusters for chart\n",
    "y_hc = hc.fit_predict(cpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average of map points\n",
    "def met1(qmap):\n",
    "    fli = []\n",
    "    for key in qmap.keys():\n",
    "        avgk1 = 0\n",
    "        avgk2 = 0\n",
    "        k1 = []\n",
    "        k2 = []\n",
    "        \n",
    "        for k in qmap[key].keys():\n",
    "            k1.append(k[0])\n",
    "            k2.append(k[1])\n",
    "        kt1 = 0\n",
    "        kt2 = 0\n",
    "        \n",
    "        for i, j in zip(k1, k2):\n",
    "            kt1 = kt1 + i\n",
    "            kt2 = kt2 + j\n",
    "        avgk1 = kt1/len(k1)\n",
    "        avgk2 = kt2/len(k2)\n",
    "        \n",
    "        print(\"The key: \", key)\n",
    "        print(\"The metric: \", avgk1, avgk2)\n",
    "        fli.append(tuple((avgk1, avgk2)))\n",
    "    return fli\n",
    "\n",
    "# distance between maps euclidean\n",
    "def met2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hfqav = met1(hfq_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hfaav = met1(hfa_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Calssifier SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load up the unique points\n",
    "hfq_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "hfq_map = load_idx(hfq_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load up the unique points\n",
    "muq_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "muq_map = load_idx(muq_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load up the unique points\n",
    "teq_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "teq_map = load_idx(teq_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load up the unique points\n",
    "alq_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "alq_map = load_idx(alq_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load up the non-unique points\n",
    "full_files = file_list(\"mapping\", \"csv\")\n",
    "# setting up all the mappings\n",
    "full_map = load_idx(full_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the labeled points\n",
    "# plot the driver points\n",
    "# get the driver details\n",
    "# sorted drivers and unsorted drivers\n",
    "def pseudo_classifier_label(inp, lab, ind_mp, ind_pt, max_size, thresh_name, loc, save):\n",
    "    \n",
    "    colorlis = ['r', 'g', 'b', 'y', 'k']\n",
    "    map_keys, map_vals = lookup(ind_pt)\n",
    "    fdi = dict()\n",
    "    \n",
    "    for o in range(len(map_keys)):\n",
    "        print(\"Currently mapping....\", map_keys[o], \" --> \", o)\n",
    "        plt.axis([0, max_size, 0, max_size])\n",
    "        cur = map_vals[o]\n",
    "        liv, labcol = [], []\n",
    "        print(\"Total no. of points to label....\", len(cur))\n",
    "        \n",
    "        # open the main map dictionary\n",
    "        for key in cur:\n",
    "            \n",
    "            lis = [0] * (len(set(lab)))\n",
    "\n",
    "            # open a point in the list\n",
    "            for pt in ind_mp[key]:\n",
    "\n",
    "                # deal only with points in the required label limit\n",
    "                if pt < lab.shape[0]: \n",
    "                    res = lab[pt]\n",
    "                    lis[res] = lis[res] + 1\n",
    "                    maxi = max(lis)\n",
    "            #print(lis)\n",
    "\n",
    "            # main map\n",
    "            # locate the index of the maximum value and plot only that\n",
    "            for i in range(len(lis)):\n",
    "                if lis[i] == maxi:\n",
    "                    #print(\"MAX: \", maxi)\n",
    "                    pts = i\n",
    "                    #print(\"MAX lab: \", pt)\n",
    "\n",
    "            \n",
    "            tuv = (key[0], key[1], pts)\n",
    "            liv.append(tuv)\n",
    "            \n",
    "            #print(\"x = \", key[0], \" y = \", key[1], \" lab = \", pts)\n",
    "            plt.plot(key[0], key[1], marker='.', color=colorlis[pts])\n",
    "            labcol.append(pts)\n",
    "                    \n",
    "        print(\"Label Analysis:-\")\n",
    "        print(\"Total no. of labels: \", len(labcol))\n",
    "\n",
    "        # determine label differences\n",
    "        ldiff = list(set(labcol))\n",
    "        cur_li = [0] * (max(ldiff)+1)\n",
    "        for thept in labcol:\n",
    "            cur_li[thept] = cur_li[thept] + 1\n",
    "        for tot in range(len(cur_li)):\n",
    "            print(\"Total no. of lab \", tot+1, \" = \", cur_li[tot])\n",
    "            percent = (cur_li[tot]/len(labcol))*100\n",
    "            print(\"Total percentage of lab \", tot+1, \" = \", percent, \"%\")\n",
    "                \n",
    "        \n",
    "        nmod = map_keys[o].rstrip(\".csv\")\n",
    "        thn = nmod + thresh_name \n",
    "        plt.title(thn)\n",
    "\n",
    "        # save the labeled map\n",
    "        if save == True:\n",
    "            os.chdir(loc)\n",
    "            plt.savefig(thn)\n",
    "            os.chdir('..')\n",
    "            print(\"map saved!!\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        fdi[map_keys[o]] = liv\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    return fdi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brake label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hfq_cl = pseudo_classifier_label(num_npy, br_remap['hf_brl'], full_map['hfn_re_id.csv'], hfq_map, \n",
    "                                 60, \"_br_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muq_cl = pseudo_classifier_label(num_npy, br_remap['mu_brl'], full_map['mun_re_id.csv'], muq_map, \n",
    "                                 60, \"_br_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_cl = pseudo_classifier_label(num_npy, br_remap['te_brl'], full_map['ten_re_id.csv'], teq_map, \n",
    "                                 60, \"_br_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_cl = pseudo_classifier_label(num_npy, br_remap['al_brl'], full_map['aln_re_id.csv'], alq_map, \n",
    "                                 80, \"_br_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfq_sd = pseudo_classifier_label(num_npy, sd_remap['hf_sdl'], full_map['hfn_re_id.csv'], hfq_map, \n",
    "                                 60, \"_sd_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muq_sd = pseudo_classifier_label(num_npy, sd_remap['mu_sdl'], full_map['mun_re_id.csv'], muq_map, \n",
    "                                 60, \"_sd_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_sd = pseudo_classifier_label(num_npy, sd_remap['te_sdl'], full_map['ten_re_id.csv'], teq_map, \n",
    "                                 60, \"_sd_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_sd = pseudo_classifier_label(num_npy, sd_remap['al_sdl'], full_map['aln_re_id.csv'], alq_map, \n",
    "                                 80, \"_sd_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Front back label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hfq_fb = pseudo_classifier_label(num_npy, fb_remap['hf_fbl'], full_map['hfn_re_id.csv'], hfq_map, \n",
    "                                 60, \"_fb_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muq_fb = pseudo_classifier_label(num_npy, fb_remap['mu_fbl'], full_map['mun_re_id.csv'], muq_map, \n",
    "                                 60, \"_fb_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_fb = pseudo_classifier_label(num_npy, fb_remap['te_fbl'], full_map['ten_re_id.csv'], teq_map, \n",
    "                                 60, \"_fb_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_fb = pseudo_classifier_label(num_npy, fb_remap['al_fbl'], full_map['aln_re_id.csv'], alq_map, \n",
    "                                 80, \"_fb_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Left right label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfq_lr = pseudo_classifier_label(num_npy, lr_remap['hf_lrl'], full_map['hfn_re_id.csv'], hfq_map, \n",
    "                                 60, \"_lr_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muq_lr = pseudo_classifier_label(num_npy, lr_remap['mu_lrl'], full_map['mun_re_id.csv'], muq_map, \n",
    "                                 60, \"_lr_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_lr = pseudo_classifier_label(num_npy, lr_remap['te_lrl'], full_map['ten_re_id.csv'], teq_map, \n",
    "                                 60, \"_lr_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_lr = pseudo_classifier_label(num_npy, lr_remap['al_lrl'], full_map['aln_re_id.csv'], alq_map, \n",
    "                                 80, \"_lr_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top bottom label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfq_tb = pseudo_classifier_label(num_npy, tb_remap['hf_tbl'], full_map['hfn_re_id.csv'], hfq_map, \n",
    "                                 60, \"_tb_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muq_tb = pseudo_classifier_label(num_npy, tb_remap['mu_tbl'], full_map['mun_re_id.csv'], muq_map, \n",
    "                                 60, \"_tb_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_tb = pseudo_classifier_label(num_npy, tb_remap['te_tbl'], full_map['ten_re_id.csv'], teq_map, \n",
    "                                 60, \"_tb_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_tb = pseudo_classifier_label(num_npy, tb_remap['al_tbl'], full_map['aln_re_id.csv'], alq_map, \n",
    "                                 80, \"_tb_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane gap label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfq_lg = pseudo_classifier_label(num_npy, lg_remap['hf_lgl'], full_map['hfn_re_id.csv'], hfq_map, \n",
    "                                 60, \"_lg_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muq_lg = pseudo_classifier_label(num_npy, lg_remap['mu_lgl'], full_map['mun_re_id.csv'], muq_map, \n",
    "                                 60, \"_lg_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_lg = pseudo_classifier_label(num_npy, lg_remap['te_lgl'], full_map['ten_re_id.csv'], teq_map, \n",
    "                                 60, \"_lg_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_lg = pseudo_classifier_label(num_npy, lg_remap['al_lgl'], full_map['aln_re_id.csv'], alq_map, \n",
    "                                 80, \"_lg_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split 2 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfq_s2 = pseudo_classifier_label(num_npy, s2_remap['hf_s2l'], full_map['hfn_re_id.csv'], hfq_map, \n",
    "                                 60, \"_s2_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muq_s2 = pseudo_classifier_label(num_npy, s2_remap['mu_s2l'], full_map['mun_re_id.csv'], muq_map, \n",
    "                                 60, \"_s2_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_s2 = pseudo_classifier_label(num_npy, s2_remap['te_s2l'], full_map['ten_re_id.csv'], teq_map, \n",
    "                                 60, \"_s2_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_s2 = pseudo_classifier_label(num_npy, s2_remap['al_s2l'], full_map['aln_re_id.csv'], alq_map, \n",
    "                                 80, \"_s2_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split 3 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hfq_s3 = pseudo_classifier_label(num_npy, s3_remap['hf_s3l'], full_map['hfn_re_id.csv'], hfq_map, \n",
    "                                 60, \"_s3_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muq_s3 = pseudo_classifier_label(num_npy, s3_remap['mu_s3l'], full_map['mun_re_id.csv'], muq_map, \n",
    "                                 60, \"_s3_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_s3 = pseudo_classifier_label(num_npy, s3_remap['te_s3l'], full_map['ten_re_id.csv'], teq_map, \n",
    "                                 60, \"_s3_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_s3 = pseudo_classifier_label(num_npy, s3_remap['al_s3l'], full_map['aln_re_id.csv'], alq_map, \n",
    "                                 80, \"_s3_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split 4 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfq_s4 = pseudo_classifier_label(num_npy, s4_remap['hf_s4l'], full_map['hfn_re_id.csv'], hfq_map, \n",
    "                                 60, \"_s4_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muq_s4 = pseudo_classifier_label(num_npy, s4_remap['mu_s4l'], full_map['mun_re_id.csv'], muq_map, \n",
    "                                 60, \"_s4_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_s4 = pseudo_classifier_label(num_npy, s4_remap['te_s4l'], full_map['ten_re_id.csv'], teq_map, \n",
    "                                 60, \"_s4_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_s4 = pseudo_classifier_label(num_npy, s4_remap['al_s4l'], full_map['aln_re_id.csv'], alq_map, \n",
    "                                 80, \"_s4_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split 5 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfq_s5 = pseudo_classifier_label(num_npy, s5_remap['hf_s5l'], full_map['hfn_re_id.csv'], hfq_map, \n",
    "                                 60, \"_s5_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muq_s5 = pseudo_classifier_label(num_npy, s5_remap['mu_s5l'], full_map['mun_re_id.csv'], muq_map, \n",
    "                                 60, \"_s5_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teq_s5 = pseudo_classifier_label(num_npy, s5_remap['te_s5l'], full_map['ten_re_id.csv'], teq_map, \n",
    "                                 60, \"_s5_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alq_s5 = pseudo_classifier_label(num_npy, s5_remap['al_s5l'], full_map['aln_re_id.csv'], alq_map, \n",
    "                                 80, \"_s5_cl2\", \"results\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing each label with driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot points\n",
    "def qdr_plot(adr, names, grtitle, grloc, grsave):\n",
    "    adrk, adrv = lookup(adr)\n",
    "    \n",
    "    # get list of labels\n",
    "    labset = []\n",
    "    colorlis = ['r', 'g', 'b', 'y', 'k']\n",
    "    nameset = names\n",
    "    \n",
    "    for tup in adrv[0]:\n",
    "        if tup[2] in labset:\n",
    "            continue\n",
    "        else:\n",
    "            labset.append(tup[2])\n",
    "    \n",
    "    labset = sorted(labset)\n",
    "    print(\"Labels are: \", labset)\n",
    "    \n",
    "    val_li = [0]*len(labset)\n",
    "    for k in range(len(val_li)):\n",
    "        val_li[k] = list()\n",
    "        \n",
    "    # open each dictionary\n",
    "    for n in range(len(adrk)):\n",
    "        cur = adrv[n]\n",
    "        cur_len = len(cur)\n",
    "        # get percent of each label\n",
    "        lab_li = [0]*len(labset)\n",
    "        \n",
    "        for val in cur:\n",
    "            lab_li[val[2]] = lab_li[val[2]] + 1\n",
    "        \n",
    "        print(\"Label count of Driver: \", adrk[n], \" : \", lab_li)\n",
    "        for k in range(len(lab_li)):\n",
    "            percent = (lab_li[k]/cur_len)*100\n",
    "            print(\"Percent of lab-\", lab_li[k], \" : \", percent)\n",
    "            val_li[k].append(np.floor(percent))\n",
    "            \n",
    "        \n",
    "        print(\"Plotting \", adrk[n], \" complete!!\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # plotting phase\n",
    "    nobr = val_li[0]\n",
    "    yesbr = val_li[1]\n",
    "    \n",
    "    plt.figure(num=None, figsize=(5, 10), dpi=200, facecolor='w', edgecolor='k')\n",
    "    x = np.arange(40)  # the label locations\n",
    "    width = 0.2  # the width of the bars\n",
    "    \n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(len(val_li)):\n",
    "        ax.barh(x+(width*i), val_li[i], width, label=nameset[i], color=colorlis[i], align='center')\n",
    "    \n",
    "    #ax.bar(x+(width*len(val_li)), val_li[len(val_li)-1], width, color='w', align='center')\n",
    "    \n",
    "    ax.autoscale(tight=True)\n",
    "    plt.axis([0, 100, -1, 41])\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels\n",
    "    ax.set_xlabel('Percentage')\n",
    "    ax.set_ylabel('Driver ID')\n",
    "    ax.set_title(grtitle)\n",
    "    ax.legend(loc=grloc)\n",
    "    \n",
    "    \n",
    "    if grsave == True:\n",
    "        os.chdir('results')\n",
    "        plt.savefig(grtitle)\n",
    "        os.chdir('..')\n",
    "        print(\"Map Saved!!\")\n",
    "        \n",
    "    plt.show()\n",
    "    return val_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brlabs = ['Non-Brake', 'Brake']\n",
    "sdlabs = ['Speed Below Avg.', 'Speed Above Avg.']\n",
    "fblabs = ['Along X (Deceleration)', 'Along X (Accelertion)']\n",
    "lrlabs = ['Acceleration along Y (Left)', 'Acceleration along Y (Right)']\n",
    "tblabs = ['Along Z (Lower Altitude)', 'Along Z (Higher Altitude)']\n",
    "lglabs = ['Left of lane center', 'Right of lane center']\n",
    "s2labs = ['Section 1', 'Section 2']\n",
    "s3labs = ['Section 1', 'Section 2', 'Section 3']\n",
    "s4labs = ['Section 1', 'Section 2', 'Section 3', 'Section 4']\n",
    "s5labs = ['Section 1', 'Section 2', 'Section 3', 'Section 4', 'Section 5']\n",
    "\n",
    "brti = '_nonbrake_vs_brake'\n",
    "sdti = '_belowavg_vs_aboveavg'\n",
    "fbti = '_backward_vs_forward'\n",
    "lrti = '_left_vs_right'\n",
    "tbti = '_low_vs_high'\n",
    "lgti = '_leftlane_vs_rightlane'\n",
    "s2ti = '_1st_vs_2nd'\n",
    "s3ti = '_1st_vs_2nd_vs_3rd'\n",
    "s4ti = '_1st_vs_2nd_vs_3rd_vs_4th'\n",
    "s5ti = '_1st_vs_2nd_vs_3rd_vs_4th_vs_5th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albr = qdr_plot(alq_cl, brlabs, 'al'+brti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alsd = qdr_plot(alq_sd, sdlabs, 'al'+sdti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfb = qdr_plot(alq_fb, fblabs, 'al'+fbti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allr = qdr_plot(alq_lr, lrlabs, 'al'+lrti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altb = qdr_plot(alq_tb, tblabs, 'al'+tbti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allg = qdr_plot(alq_lg, lglabs, 'al'+lgti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als2 = qdr_plot(alq_s2, s2labs, 'al'+s2ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als3 = qdr_plot(alq_s3, s3labs, 'al'+s3ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als4 = qdr_plot(alq_s4, s4labs, 'al'+s4ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als5 = qdr_plot(alq_s5, s5labs, 'al'+s5ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing each label and distraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot points\n",
    "def qdi_plot(adi, grtitle, grloc, grsave):\n",
    "    \n",
    "    x = np.arange(40)\n",
    "    width = 0.2\n",
    "    \n",
    "    for i in range(len(adi[0])):\n",
    "        plt.figure(num=None, figsize=(10, 5), dpi=100, facecolor='w', edgecolor='k')\n",
    "        ax = plt.subplot(111)\n",
    "        al, bl, cl = [], [], []\n",
    "        aw, bw, cw = 0, 0, 0\n",
    "        pt = 1\n",
    "        \n",
    "        for a, b, c in zip(adi[0][i], adi[1][i], adi[2][i]):\n",
    "            al.append(a)\n",
    "            bl.append(b)\n",
    "            cl.append(c)\n",
    "            \n",
    "            li = [a, b, c]\n",
    "            lar = max(li)\n",
    "            sma = min(li)\n",
    "            print(li)\n",
    "            if a == lar:\n",
    "                print(\"Driver \", pt, \"most activated under distraction -Hands-Free- labeled by: \", grtitle[i])\n",
    "                aw = aw + 1\n",
    "            \n",
    "            if b == lar:\n",
    "                print(\"Driver \", pt, \"most activated under distraction -Music- labeled by: \", grtitle[i])\n",
    "                bw = bw + 1\n",
    "                \n",
    "            if c == lar:\n",
    "                print(\"Driver \", pt, \"most activated under distraction -Text- labeled by: \", grtitle[i])\n",
    "                cw = cw + 1\n",
    "                \n",
    "            if a == sma:\n",
    "                print(\"Driver \", pt, \"least activated under distraction -Hands-Free- labeled by: \", grtitle[i])\n",
    "            \n",
    "            if b == sma:\n",
    "                print(\"Driver \", pt, \"least activated under distraction -Music- labeled by: \", grtitle[i])\n",
    "                \n",
    "            if c == sma:\n",
    "                print(\"Driver \", pt, \"least activated under distraction -Text- labeled by: \", grtitle[i])\n",
    "                \n",
    "            pt = pt + 1\n",
    "            \n",
    "        ax.bar(x, al, width, label='Hands-Free', color='r', align='center')\n",
    "        ax.bar(x+width, bl, width, label='Music', color='g', align='center')\n",
    "        ax.bar(x+width*2, cl, width, label='Text', color='b', align='center')\n",
    "\n",
    "        ax.autoscale(tight=True)\n",
    "        plt.axis([-1, 41, 0, 100])\n",
    "\n",
    "        # Add some text for labels, title and custom x-axis tick labels\n",
    "        ax.set_xlabel('Driver ID')\n",
    "        ax.set_ylabel('Percentage')\n",
    "        ax.set_title(grtitle[i])\n",
    "        ax.legend(loc=grloc)\n",
    "\n",
    "        print()\n",
    "        winli = [aw, bw, cw]\n",
    "        maxli = max(winli)\n",
    "        minli = min(winli)\n",
    "        maxper = (maxli/40)*100\n",
    "        minper = (minli/40)*100\n",
    "        print(winli, ((aw/40)*100), ((bw/40)*100), ((cw/40)*100))\n",
    "        print(maxli, minli, maxper, minper)\n",
    "        \n",
    "        if aw == maxli:\n",
    "            print(\"Most drivers tend to be activated under -Hands-Free-: \", maxli, maxper, \" under label \", grtitle[i])\n",
    "\n",
    "        if bw == maxli:\n",
    "            print(\"Most drivers tend to be activated under -Music- \", maxli, maxper, \" under label \", grtitle[i])\n",
    "\n",
    "        if cw == maxli:\n",
    "            print(\"Most drivers tend to be activated under -Text-:\", maxli, maxper, \" under label \", grtitle[i])\n",
    "\n",
    "        if aw == minli:\n",
    "            print(\"Least drivers tend to be activated under -Hands-Free-:\", minli, minper, \" under label \", grtitle[i])\n",
    "\n",
    "        if bw == minli:\n",
    "            print(\"Least drivers tend to be activated under -Music-:\", minli, minper, \" under label \", grtitle[i])\n",
    "\n",
    "        if cw == minli:\n",
    "            print(\"Least drivers tend to be activated under -Text-: \", minli, minper, \" under label \", grtitle[i])\n",
    "            \n",
    "        if grsave == True:\n",
    "            os.chdir('results')\n",
    "            plt.savefig(grtitle[i])\n",
    "            os.chdir('..')\n",
    "            print(\"Map Saved!!\")\n",
    "\n",
    "        plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "# combining labels and plotting individual distractions\n",
    "def indi_di(adi, names, grloc, grsave):\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    \n",
    "    colorlis = ['r', 'g', 'b', 'y', 'k']\n",
    "    distract = ['hf', 'mu', 'te']\n",
    "    \n",
    "    for li in range(len(adi)):\n",
    "        x = np.arange(40)\n",
    "        plt.figure(num=None, figsize=(10, 5), dpi=200, facecolor='w', edgecolor='k')\n",
    "        ax = plt.subplot(111)\n",
    "            \n",
    "        for i in range(len(adi[li])):\n",
    "            dist = adi[li]\n",
    "            width = 0.2\n",
    "            ax.bar(x+width*i, dist[i], width, label=names[i], color=colorlis[i], align='center')\n",
    "\n",
    "        ax.autoscale(tight=True)\n",
    "        plt.axis([-1, 41, 0, 100])\n",
    "\n",
    "        # Add some text for labels, title and custom x-axis tick labels\n",
    "        ax.set_xlabel('Driver ID')\n",
    "        ax.set_ylabel('Percentage')\n",
    "        \n",
    "        grtitle = distract[li]\n",
    "        for n in names:\n",
    "            grtitle = grtitle + '_' + n\n",
    "            \n",
    "        ax.set_title(grtitle)\n",
    "        ax.legend(loc=grloc)\n",
    "\n",
    "        if grsave == True:\n",
    "            os.chdir('results')\n",
    "            plt.savefig(grtitle)\n",
    "            os.chdir('..')\n",
    "            print(\"Map Saved!!\")\n",
    "\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_sp(alist):\n",
    "    a, b, c = [], [], []\n",
    "    \n",
    "    for i in alist:\n",
    "        ptr = 0\n",
    "        ta, tb, tc = [], [], []\n",
    "        for j in i:\n",
    "            if ptr < 40:\n",
    "                ta.append(j)\n",
    "            if ptr < 80 and ptr >= 40:\n",
    "                tb.append(j)\n",
    "            if ptr >= 80:\n",
    "                tc.append(j)\n",
    "            ptr = ptr + 1\n",
    "        \n",
    "        a.append(ta)\n",
    "        b.append(tb)\n",
    "        c.append(tc)\n",
    "        \n",
    "    return [a, b, c]\n",
    "\n",
    "\n",
    "abr = func_sp(albr)\n",
    "asd = func_sp(alsd)\n",
    "afb = func_sp(alfb)\n",
    "alr = func_sp(allr)\n",
    "atb = func_sp(altb)\n",
    "alg = func_sp(allg)\n",
    "as2 = func_sp(als2)\n",
    "as3 = func_sp(als3)\n",
    "as4 = func_sp(als4)\n",
    "as5 = func_sp(als5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qbr = [hfbr, mubr, tebr]\n",
    "qsd = [hfsd, musd, tesd]\n",
    "qfb = [hffb, mufb, tefb]\n",
    "qlr = [hflr, mulr, telr]\n",
    "qtb = [hftb, mutb, tetb]\n",
    "qlg = [hflg, mulg, telg]\n",
    "qs2 = [hfs2, mus2, tes2]\n",
    "qs3 = [hfs3, mus3, tes3]\n",
    "qs4 = [hfs4, mus4, tes4]\n",
    "qs5 = [hfs5, mus5, tes5]\n",
    "\n",
    "qbr_ti = ['Non_Brake','Brake']\n",
    "qsd_ti = ['Below_Average_Speed', 'Above_Average_Speed']\n",
    "qfb_ti = ['Backward_Deceleration', 'Forward_Acceleration']\n",
    "qlr_ti = ['Left_Acceleration', 'Right_Acceleration']\n",
    "qtb_ti = ['Lower_Altitude_Acceleration', 'Higher_Altitude_Acceleration']\n",
    "qlg_ti = ['Left_Center_Lane', 'Right_Center_Lane']\n",
    "qs2_ti = ['Sp2_1st', 'Sp2_2nd']\n",
    "qs3_ti = ['Sp3_1st', 'Sp3_2nd', 'Sp3_3rd']\n",
    "qs4_ti = ['Sp4_1st', 'Sp4_2nd', 'Sp4_3rd', 'Sp4_4th']\n",
    "qs5_ti = ['Sp5_1st', 'Sp5_2nd', 'Sp5_3rd', 'Sp5_4th', 'Sp5_5th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indi_di(abr, qbr_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdi_plot(abr, qbr_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_di(asd, qsd_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdi_plot(asd, qsd_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acceleration x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_di(afb, qfb_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdi_plot(afb, qfb_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acceleration y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_di(alr, qlr_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdi_plot(alr, qlr_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# acceleration z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_di(atb, qtb_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdi_plot(atb, qtb_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lane gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_di(alg, qlg_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdi_plot(alg, qlg_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_di(as2, qs2_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdi_plot(as2, qs2_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_di(as3, qs3_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdi_plot(as3, qs3_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_di(as4, qs4_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdi_plot(as4, qs4_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indi_di(as5, qs5_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdi_plot(as5, qs5_ti, 'upper right', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the labeled points\n",
    "# plot the driver points\n",
    "# get the driver details\n",
    "# sorted drivers and unsorted drivers\n",
    "def pseudo_classifier_data(inp, lab, ind_mp, ind_pt, max_size, thresh_name, loc, save):\n",
    "    \n",
    "    colorlis = ['r', 'g', 'b', 'y', 'k']\n",
    "    map_keys, map_vals = lookup(ind_pt)\n",
    "    fdi = dict()\n",
    "    \n",
    "    for o in range(39, len(map_keys)):\n",
    "        print(\"Currently mapping....\", map_keys[o], \" --> \", o)\n",
    "        f0, f1 = 0, 0\n",
    "        #plt.axis([0, max_size, 0, max_size])\n",
    "        cur = map_vals[o]\n",
    "        liv, labcol = [], []\n",
    "        print(\"Total no. of points to label....\", len(cur))\n",
    "        \n",
    "        # open the main map dictionary\n",
    "        for key in ind_mp.keys():\n",
    "            # deal only with key having something in the list\n",
    "            if(len(ind_mp[key]) != 0):\n",
    "                lis = [0] * (len(set(lab)))\n",
    "\n",
    "                # open a point in the list\n",
    "                for pt in ind_mp[key]:\n",
    "\n",
    "                    # deal only with points in the required label limit\n",
    "                    if pt < lab.shape[0]: \n",
    "                        res = lab[pt]\n",
    "                        lis[res] = lis[res] + 1\n",
    "                        maxi = max(lis)\n",
    "                #print(lis)\n",
    "                \n",
    "                # main map\n",
    "                # locate the index of the maximum value and plot only that\n",
    "                for i in range(len(lis)):\n",
    "                    if lis[i] == maxi:\n",
    "                        #print(\"MAX: \", maxi)\n",
    "                        pts = i\n",
    "                        #print(\"MAX lab: \", pt)\n",
    "                \n",
    "                # mini map\n",
    "                for kk in cur:\n",
    "                    if kk[0] == key[0] and kk[1] == key[1]:\n",
    "                        f0 = 1\n",
    "                        f1 = 1\n",
    "                        break\n",
    "                \n",
    "                if f0 == 1 and f1 == 1:\n",
    "                    tuv = (key[0], key[1], pts)\n",
    "                    liv.append(tuv)\n",
    "                    f0, f1 = 0, 0\n",
    "                    print(\"x = \", key[0], \" y = \", key[1], \" lab = \", pts)\n",
    "                    plt.plot(key[0], key[1], marker='.', color=colorlis[pts])\n",
    "                    labcol.append(pts)\n",
    "                    \n",
    "        print(\"Label Analysis:-\")\n",
    "        print(\"Total no. of labels: \", len(labcol))\n",
    "\n",
    "        # determine label differences\n",
    "        ldiff = list(set(labcol))\n",
    "        cur_li = [0] * (max(ldiff)+1)\n",
    "        for thept in labcol:\n",
    "            cur_li[thept] = cur_li[thept] + 1\n",
    "        for tot in range(len(cur_li)):\n",
    "            print(\"Total no. of lab \", tot+1, \" = \", cur_li[tot])\n",
    "            percent = (cur_li[tot]/len(labcol))*100\n",
    "            print(\"Total percentage of lab \", tot+1, \" = \", percent, \"%\")\n",
    "                \n",
    "            \n",
    "        \n",
    "        nmod = map_keys[o].rstrip(\".csv\")\n",
    "        thn = nmod + thresh_name \n",
    "        plt.title(thn)\n",
    "\n",
    "        # save the labeled map\n",
    "        if save == True:\n",
    "            os.chdir(loc)\n",
    "            plt.savefig(thn)\n",
    "            os.chdir('..')\n",
    "            print(\"map saved!!\")\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        fdi[map_keys[o]] = liv\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    return fdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfq_map['hfq_map9.csv'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining Segmented maps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the resultant mappings\n",
    "map_seg = file_list(\"mapping\", \"csv\")\n",
    "\n",
    "# setting up all the mappings\n",
    "seg_ind = load_idx(map_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_key(keyno, thedict):\n",
    "    retdict = dict()\n",
    "    thek, thev = lookup(thedict)\n",
    "    \n",
    "    for i in range(len(thek)):\n",
    "        if i == keyno:\n",
    "            continue\n",
    "        else:\n",
    "            retdict[thek[i]] = thev[i]\n",
    "    return retdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segwt = dict()\n",
    "segwt['aln_wt'] = tr_wt['aln_re_wt.npy'][:40, :40, :]\n",
    "segwt['hfn_wt'] = tr_wt['hfn_re_wt.npy'][:30, :30, :]\n",
    "segwt['mun_wt'] = tr_wt['mun_re_wt.npy'][:30, :30, :]\n",
    "segwt['ten_wt'] = tr_wt['ten_re_wt.npy'][:30, :30, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine the brake label\n",
    "all_1lab_noobj(br_remap, segwt, seg_ind, \"results_dr\", \"_brs\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine the speed label\n",
    "all_1lab_noobj(sd_remap, segwt, seg_ind, \"results_dr\", \"_sds\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine the foward and backward motion\n",
    "all_1lab_noobj(fb_remap, segwt, seg_ind, \"results_dr\", \"_fbs\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine the left and right motion\n",
    "all_1lab_noobj(lr_remap, segwt, seg_ind, \"results_dr\", \"_lrs\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#examine the top and bottom motion\n",
    "all_1lab_noobj(tb_remap, segwt, seg_ind, \"results_dr\", \"_tbs\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine the lane gap\n",
    "all_1lab_noobj(lg_remap, segwt, seg_ind, \"results_dr\", \"_lgs\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to analyse SOM results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions that help in cluster analysis\n",
    "\n",
    "# function that returns values in a certain space on the map\n",
    "def clus_ret(x1, x2, y1, y2, keyset):\n",
    "    '''\n",
    "    x1, x2, y1, y2: x and y coordinates\n",
    "    keyset: set of indices\n",
    "    '''\n",
    "    \n",
    "    # get a set of map points\n",
    "    cl = []\n",
    "    for i in range(x1, x2):\n",
    "        for j in range(y1, y2):\n",
    "            tup = (i, j)\n",
    "            cl.append(tup)\n",
    "    \n",
    "    # refine the set of map points\n",
    "    recl = []\n",
    "    for c1 in keyset.keys():\n",
    "        for c2 in cl:\n",
    "            if c1 == c2:\n",
    "                recl.append(c1)\n",
    "\n",
    "    fi = []\n",
    "    fi_win = defaultdict(list)\n",
    "    for cc in recl:\n",
    "        #print(cc)\n",
    "        for l in keyset[cc]:\n",
    "            fi_win[cc].append(l)\n",
    "            fi.append(l)\n",
    "    \n",
    "    #print(fi)\n",
    "    fi.sort()\n",
    "    print(len(fi))\n",
    "    return fi_win, fi\n",
    "\n",
    "\n",
    "\n",
    "# function to return labels of a particular point\n",
    "def lab_ret(id1, id2, idx, lab, name):\n",
    "    \n",
    "    li = idx[name][id1, id2]\n",
    "    print(\"Labels in \", id1, \" and \", id2, \" of \", name, \" are:\")\n",
    "    print(lab[li, 0])\n",
    "    return lab[li, 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# function to return and display dictionaries of dataframes of the respective labels\n",
    "def join_dflab(df_dict, lab_dict):\n",
    "    \n",
    "    # get the keys and values of required dicts\n",
    "    df_k, df_v = sep_dict(df_dict)\n",
    "    la_k, la_v = sep_dict(lab_dict)\n",
    "    \n",
    "    # combine label with dataframe as a new feature\n",
    "    jo_v = []\n",
    "    \n",
    "    for i in range(len(df_k)):\n",
    "        df = df_v[i]\n",
    "        la = la_v[i]\n",
    "        \n",
    "        # convert label to dataframe\n",
    "        la_df = pd.DataFrame({la_k[i] : la[:, 0]})\n",
    "        \n",
    "        # do the joining\n",
    "        joint = pd.concat((df, la_df), axis=1)\n",
    "        jo_v.append(joint)\n",
    "        \n",
    "    # return that dataframe dictionary\n",
    "    dflab_dict = join_dict(df_k, jo_v)\n",
    "    return dflab_dict\n",
    "\n",
    "\n",
    "# function to show only keyset points in the labeled dataframe\n",
    "def show_dflab(dflab1, keyset1):\n",
    "    \n",
    "    prev = keyset1[0]\n",
    "    cur = prev + 1\n",
    "    new_df = pd.DataFrame(dflab1.iloc[prev:cur, :])\n",
    "    \n",
    "    for key in range(1, len(keyset1)):\n",
    "        prev = keyset1[key]\n",
    "        cur = prev + 1\n",
    "        nex = pd.DataFrame(dflab1.iloc[prev:cur, :])\n",
    "        new_df = new_df.append(nex, ignore_index=True)\n",
    "    \n",
    "    key_df = pd.DataFrame({\"og_keys\" : keyset1})\n",
    "    \n",
    "    fin_df = pd.concat((key_df, new_df), axis=1)\n",
    "    return fin_df\n",
    "\n",
    "\n",
    "'''# plotting two results against each other as a comparision\n",
    "def plot_aga(no1, no2):\n",
    "    \n",
    "    #no1: larger cluster\n",
    "    #no2: smaller cluster\n",
    "    \n",
    "    \n",
    "    # keep the larger set outside\n",
    "    count = 0\n",
    "    plt.axis([0, 31, 0, 31])\n",
    "    \n",
    "    for n1 in no1.keys():\n",
    "        for n2 in no2.keys():\n",
    "            for l1 in no1[n1]:\n",
    "                for l2 in no2[n2]:\n",
    "                    if l1 == l2:\n",
    "                        count = count + 1\n",
    "                        plt.plot(n1[0]+0.5, n1[1]+0.5, 'r.')\n",
    "                        \n",
    "    print(\"Number of matched points....\", count)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# plot points after searching on the map\n",
    "def search(pt, mapp):\n",
    "    plt.axis([0, 31, 0, 31])\n",
    "    for cc in mapp.keys():\n",
    "        for li in mapp[cc]:\n",
    "            for p in pt:\n",
    "                if li == p:\n",
    "                    print(cc)\n",
    "                    plt.plot(cc[0]+0.5, cc[1]+0.5, 'b.')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# return a list of points that need to be searched                \n",
    "def c_search(start, last):\n",
    "    li = []\n",
    "    for i in range(start, last+1):\n",
    "        li.append(i)\n",
    "        \n",
    "    return li\n",
    "\n",
    "# function that joins clusters\n",
    "def join(m1, m2):\n",
    "    di = {**m1, **m2}\n",
    "    return di'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Piecewise Label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label only splits of data\n",
    "## NOTE: SOM mapping already complete\n",
    "def pieces_1lab(lab_dict, ptr_dict, weight_dict, idx_dict, name, loc, thresh, save):\n",
    "    \n",
    "    la_k, la_v = sep_dict(lab_dict)\n",
    "    wt_k, wt_v = sep_dict(weight_dict)\n",
    "    id_k, id_v = sep_dict(idx_dict)\n",
    "    pt_k, pt_v = sep_dict(ptr_dict)\n",
    "    \n",
    "    # set the weights first\n",
    "    for wtid in range(len(wt_k)):\n",
    "        # current values:\n",
    "        cur_la = la_v[wtid]\n",
    "        cur_wt = wt_v[wtid]\n",
    "        cur_id = id_v[wtid]\n",
    "        cur_pt = pt_v[wtid]\n",
    "        \n",
    "        save_n = wt_k[wtid].rstrip('.npy')\n",
    "        \n",
    "        print(\"Starting \", wt_k[wtid], ' ...')\n",
    "        \n",
    "        # get the junctions\n",
    "        junc = ret_junc(cur_pt)\n",
    "        start = junc[0]\n",
    "        \n",
    "        for jun in range(1, len(junc[:(len(junc)-1)])):\n",
    "            end = junc[jun]\n",
    "            \n",
    "            jncs = str(start) + \"x\" + str(end)\n",
    "            # new name of the map\n",
    "            fname = \"plt_\" + save_n + \"_pie_\" + name + jncs\n",
    "\n",
    "            # plot the mapping\n",
    "            if thresh == True:\n",
    "                lab_thresh(cur_la[start:end], cur_id, cur_wt.shape[0], fname, loc, save)\n",
    "            else:\n",
    "                lab_nothresh(cur_la, cur_id, cur_wt.shape[0], fname, loc, save)\n",
    "                    \n",
    "            start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# piece wise of all data for brake label\n",
    "pieces_1lab(br_red, sp_red, load_wts, idd_map, \"_brl\", \"weights\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing some code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.axis([0, 11, -1, 10])\n",
    "colorss = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'}\n",
    "\n",
    "si = 100\n",
    "decvals = np.linspace(0, 0.85, si)\n",
    "\n",
    "for i in range(si):\n",
    "    cint = decvals[i]\n",
    "    cstr = str(cint)\n",
    "    #print(cidx, ' ---> ', cstr)\n",
    "    plt.plot(i, 0, marker='s', color=cstr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(i, 0, marker='s', color='0.85')\n",
    "plt.savefig(\"uwu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### functions that plot som map cluster points\n",
    "\n",
    "# function to load som and label values but not plot labels\n",
    "def som_label_noplot(data, label, loc, wt):\n",
    "    # data: dataset that will be used\n",
    "    # label: 1d label to use\n",
    "    # loc: name of file that stores the results and data to be loaded\n",
    "    # wt: name of the weight file\n",
    "    \n",
    "    sig = np.random.rand(1)\n",
    "    lr = np.random.rand(1)\n",
    "    \n",
    "    # create the som object\n",
    "    mine = mySOM(size, size, data.shape[1], sig, lr)\n",
    "    \n",
    "    # set the weights\n",
    "    mine.load_weights(loc, wt)\n",
    "    \n",
    "    # show the gray map (some understanding??)\n",
    "    plt.bone()\n",
    "    mapping = mine.distance_map().T\n",
    "    plt.pcolor(mapping)\n",
    "    plt.show()\n",
    "\n",
    "    # collect map to data samples, map to data index\n",
    "    win, ind = mine.win_map(data)\n",
    "    \n",
    "    return win, ind \n",
    "\n",
    "\n",
    "br_size = 22\n",
    "# function to load som and label values but not plot labels\n",
    "def som_label_brplot(data, label, loc, wt):\n",
    "    # data: dataset that will be used\n",
    "    # label: 1d label to use\n",
    "    # loc: name of file that stores the results and data to be loaded\n",
    "    # wt: name of the weight file\n",
    "    \n",
    "    sig = np.random.rand(1)\n",
    "    lr = np.random.rand(1)\n",
    "    \n",
    "    size = br_size\n",
    "    # create the som object\n",
    "    mine = mySOM(size, size, data.shape[1], sig, lr)\n",
    "    \n",
    "    # set the weights\n",
    "    mine.load_weights(loc, wt)\n",
    "    \n",
    "    # show the gray map (some understanding??)\n",
    "    plt.bone()\n",
    "    mapping = mine.distance_map().T\n",
    "    plt.pcolor(mapping)\n",
    "    plt.show()\n",
    "\n",
    "    # collect map to data samples, map to data index\n",
    "    win, ind = mine.win_map(data)\n",
    "    \n",
    "    return win, ind \n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "# excel storage fail\n",
    "'''\n",
    "import xlwt\n",
    "from xlwt import Workbook\n",
    "\n",
    "# function to write all respective excel notes\n",
    "def store_ind(cur_idx, fname):\n",
    "    \n",
    "    # create a workbook\n",
    "    wb = Workbook()\n",
    "    \n",
    "    # create an excel sheet\n",
    "    wb_sheet = wb.add_sheet('sheet_1')\n",
    "    \n",
    "    # excel sheet index\n",
    "    idx1 = 0\n",
    "    \n",
    "    for key in cur_idx.keys():\n",
    "        cur_key = key\n",
    "        cur_lis = cur_idx[key]\n",
    "        \n",
    "        # col 1 --> key 1\n",
    "        idx2 = 0\n",
    "        # create entry for key column\n",
    "        wb_sheet.write(idx1, idx2, str(cur_key[0]))\n",
    "        \n",
    "        \n",
    "        # col 2 --> key 2\n",
    "        idx2 = 1\n",
    "        # create entry for key column\n",
    "        wb_sheet.write(idx1, idx2, str(cur_key[1]))\n",
    "        \n",
    "        \n",
    "        # col 3 --> len(list)\n",
    "        idx2 = 2\n",
    "        # length of the particlar list\n",
    "        lis_si = len(cur_lis)\n",
    "        # create entry for value column\n",
    "        wb_sheet.write(idx1, idx2, str(lis_si))\n",
    "        \n",
    "        \n",
    "        # col 4 ++\n",
    "        idx2 = 3\n",
    "        for i in range(lis_si):\n",
    "            wb_sheet.write(idx1, idx2, str(cur_lis[i]))\n",
    "            idx2 = idx2 + 1\n",
    "            \n",
    "        # go to next row\n",
    "        idx1 = idx1 + 1\n",
    "        \n",
    "    name = fname + \".xls\"\n",
    "    wb.save(name)\n",
    "    print(\"File \", name, \" has been saved\")\n",
    "'''\n",
    "# class storage fail\n",
    "'''class store_idx(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # key is from the map\n",
    "        # cnt is started from 0\n",
    "        self.key_cnt = 0\n",
    "        self.key_col = []\n",
    "        self.rec_col = []\n",
    "        self.all_lis = []\n",
    "    \n",
    "    \n",
    "    # function to create the key rec table\n",
    "    def add_pt(self, key, lis):\n",
    "        # key --> rec no.\n",
    "        self.key_val = key\n",
    "        self.key_cnt = self.key_cnt + 1\n",
    "        \n",
    "        # append key and value to list\n",
    "        self.key_col.append(self.key_val)\n",
    "        self.off_col.append(self.key_cnt)\n",
    "        \n",
    "        # get and set values to the all_list\n",
    "        self.lis_val = lis\n",
    "        self.all_lis.append(self.lis_val)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # write up the file containing the whole list i.e the all_list\n",
    "    def wrt_li(self, fname):\n",
    "        # create and open file\n",
    "        txtname = fname + \".txt\"\n",
    "        f = open(txtname, \"w+\")\n",
    "        \n",
    "        # write the list\n",
    "        for li_val in self.all_lis:\n",
    "            store = str(li_val) + \" \"\n",
    "            f.write(store) \n",
    "            \n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "    # \n",
    "    # call only after all points have been added!\n",
    "    def ret_keyoff(self):\n",
    "        return self.key_col, self.off_col\n",
    "        \n",
    "    # search and locate a list\n",
    "    def sea_li(self, fname, sea_key):\n",
    "        # get the index of the offset in the table\n",
    "        for kk in range(len(self.key_col)):\n",
    "            if sea_key == self.key_col[kk]:\n",
    "                got_pt = kk\n",
    "                \n",
    "        self.offsets = self.off_col[got_pt]\n",
    "        \n",
    "        txtname = fname + \".txt\"\n",
    "        f = open(txtname, \"r+\")\n",
    "        \n",
    "        # read the file at the start and end\n",
    "        val = f.readlines(self.offsets[1])\n",
    "        \n",
    "        return val'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
